{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-ratio transformations\n",
    "def clr_inv(p):\n",
    "    z = tf.math.log(p)\n",
    "    return z - tf.reduce_mean(z, axis=1)[:, tf.newaxis]\n",
    "\n",
    "def clr_forward(z, axis=1):\n",
    "    return tf.nn.softmax(z, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_resnet(num_classes, depth=28, width=2):\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "    for _ in range(depth // 6):\n",
    "        x = layers.Conv2D(16 * width, (3, 3), padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)  # Use Keras's ReLU layer\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader combining TF CIFAR datasets with custom labels\n",
    "def load_combined_dataset(cifar_version, label_file, num_classes):\n",
    "    \"\"\"\n",
    "    Combine CIFAR image data from TensorFlow with labels from a .npy file.\n",
    "\n",
    "    Args:\n",
    "        cifar_version (str): \"cifar10\" or \"cifar100\" to select the dataset.\n",
    "        label_file (str): Path to the .npy file containing labels.\n",
    "        num_classes (int): Number of classes (10 or 100).\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset ready for training/testing.\n",
    "    \"\"\"\n",
    "    # Load image data\n",
    "    if cifar_version == \"cifar10\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar10.load_data()\n",
    "    elif cifar_version == \"cifar100\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar100.load_data()\n",
    "    else:\n",
    "        raise ValueError(\"cifar_version must be 'cifar10' or 'cifar100'.\")\n",
    "\n",
    "    # Load labels from .npy file\n",
    "    label_data = np.load(label_file, allow_pickle=True).item()\n",
    "    if 'clean_label' not in label_data:\n",
    "        raise ValueError(\"Label file must contain 'clean_label' key.\")\n",
    "\n",
    "    # Normalize image data and convert labels\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    y_train = tf.keras.utils.to_categorical(label_data['clean_label'], num_classes)\n",
    "\n",
    "    # Create TensorFlow dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian noise (manual implementation)\n",
    "def add_gaussian_noise(inputs, mean=0.0, stddev=0.1):\n",
    "    noise = tf.random.normal(shape=tf.shape(inputs), mean=mean, stddev=stddev)\n",
    "    return inputs + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(model, images, labels, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        smoothed_targets = clr_forward(labels)\n",
    "        logit_targets = clr_inv(smoothed_targets)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(logit_targets, logits))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train_model(train_ds, epochs=10, num_classes=10):\n",
    "    model = wide_resnet(num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # Define metrics\n",
    "    accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    precision_metric = tf.keras.metrics.Precision()\n",
    "    recall_metric = tf.keras.metrics.Recall()\n",
    "\n",
    "    # DataFrame to store metrics\n",
    "    metrics_df = pd.DataFrame(columns=[\"Epoch\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        epoch_loss_sum = 0\n",
    "        num_steps = 0\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        accuracy_metric.reset_states()\n",
    "        precision_metric.reset_states()\n",
    "        recall_metric.reset_states()\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_ds):\n",
    "            # Perform training step\n",
    "            loss = train_step(model, images, labels, optimizer)\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss_sum += loss.numpy()\n",
    "            num_steps += 1\n",
    "            predictions = model(images, training=False)\n",
    "            accuracy_metric.update_state(labels, predictions)\n",
    "            precision_metric.update_state(labels, predictions)\n",
    "            recall_metric.update_state(labels, predictions)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        # Compute metrics for the epoch\n",
    "        avg_loss = epoch_loss_sum / num_steps\n",
    "        epoch_accuracy = accuracy_metric.result().numpy()\n",
    "        epoch_precision = precision_metric.result().numpy()\n",
    "        epoch_recall = recall_metric.result().numpy()\n",
    "        epoch_f1_score = 2 * (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall + 1e-7)\n",
    "\n",
    "        # Display metrics\n",
    "        print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}, \"\n",
    "              f\"Accuracy: {epoch_accuracy:.4f}, \"\n",
    "              f\"Precision: {epoch_precision:.4f}, \"\n",
    "              f\"Recall: {epoch_recall:.4f}, \"\n",
    "              f\"F1-Score: {epoch_f1_score:.4f}\")\n",
    "\n",
    "        # Save metrics to DataFrame\n",
    "        metrics_df = metrics_df.append({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Loss\": avg_loss,\n",
    "            \"Accuracy\": epoch_accuracy,\n",
    "            \"Precision\": epoch_precision,\n",
    "            \"Recall\": epoch_recall,\n",
    "            \"F1-Score\": epoch_f1_score\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        # Save checkpoint with proper extension\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        model.save_weights(f\"checkpoints/sgn_epoch_{epoch}.weights.h5\")\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n",
    "\n",
    "    # Save metrics to a CSV file\n",
    "    metrics_df.to_csv(\"training_metrics.csv\", index=False)\n",
    "    print(\"Metrics saved to 'training_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Step 0, Loss: -0.30526530742645264\n",
      "Step 10, Loss: -1.2208292484283447\n",
      "Step 20, Loss: -0.24567633867263794\n",
      "Step 30, Loss: 0.16392982006072998\n",
      "Step 40, Loss: 1.118992567062378\n",
      "Step 50, Loss: -0.6978888511657715\n",
      "Step 60, Loss: -1.0975253582000732\n",
      "Step 70, Loss: -0.07976388931274414\n",
      "Step 80, Loss: 1.1383073329925537\n",
      "Step 90, Loss: -0.7111796140670776\n",
      "Step 100, Loss: -1.5299311876296997\n",
      "Step 110, Loss: 0.1551665961742401\n",
      "Step 120, Loss: -0.9019607305526733\n",
      "Step 130, Loss: 0.547715425491333\n",
      "Step 140, Loss: 0.0065004825592041016\n",
      "Step 150, Loss: 1.1441545486450195\n",
      "Step 160, Loss: 0.8311232328414917\n",
      "Step 170, Loss: -0.36357927322387695\n",
      "Step 180, Loss: -1.7897570133209229\n",
      "Step 190, Loss: -1.4610923528671265\n",
      "Step 200, Loss: 0.7477368116378784\n",
      "Step 210, Loss: -0.05420875549316406\n",
      "Step 220, Loss: -0.8020827174186707\n",
      "Step 230, Loss: 0.5844733119010925\n",
      "Step 240, Loss: -0.35730183124542236\n",
      "Step 250, Loss: -0.8903634548187256\n",
      "Step 260, Loss: 0.557440996170044\n",
      "Step 270, Loss: 0.4032473564147949\n",
      "Step 280, Loss: -0.19271790981292725\n",
      "Step 290, Loss: -1.4866342544555664\n",
      "Step 300, Loss: -1.132056713104248\n",
      "Step 310, Loss: -0.35279572010040283\n",
      "Step 320, Loss: -0.7019930481910706\n",
      "Step 330, Loss: -0.44704246520996094\n",
      "Step 340, Loss: -0.6589459180831909\n",
      "Step 350, Loss: -0.5626136064529419\n",
      "Step 360, Loss: 0.91856449842453\n",
      "Step 370, Loss: 0.5706506967544556\n",
      "Step 380, Loss: -1.3643825054168701\n",
      "Step 390, Loss: 2.1921401023864746\n",
      "Step 400, Loss: -0.7519921064376831\n",
      "Step 410, Loss: -0.6805294752120972\n",
      "Step 420, Loss: -0.6919366717338562\n",
      "Step 430, Loss: -0.9716986417770386\n",
      "Step 440, Loss: -1.7428377866744995\n",
      "Step 450, Loss: -0.24567508697509766\n",
      "Step 460, Loss: -0.6433184146881104\n",
      "Step 470, Loss: 0.5436472296714783\n",
      "Step 480, Loss: -1.2355273962020874\n",
      "Step 490, Loss: 0.428226113319397\n",
      "Step 500, Loss: 0.22825366258621216\n",
      "Step 510, Loss: -0.5279639959335327\n",
      "Step 520, Loss: -0.7423800230026245\n",
      "Step 530, Loss: 1.1517945528030396\n",
      "Step 540, Loss: -0.20214110612869263\n",
      "Step 550, Loss: 1.8140921592712402\n",
      "Step 560, Loss: -0.9634512662887573\n",
      "Step 570, Loss: 0.43417447805404663\n",
      "Step 580, Loss: -0.6966134309768677\n",
      "Step 590, Loss: 0.07309991121292114\n",
      "Step 600, Loss: 0.399741530418396\n",
      "Step 610, Loss: -1.996126651763916\n",
      "Step 620, Loss: 1.1580634117126465\n",
      "Step 630, Loss: 0.9453389644622803\n",
      "Step 640, Loss: 0.9053343534469604\n",
      "Step 650, Loss: 0.9350883960723877\n",
      "Step 660, Loss: -0.28535687923431396\n",
      "Step 670, Loss: 0.2731688618659973\n",
      "Step 680, Loss: 1.4306029081344604\n",
      "Step 690, Loss: 1.1562819480895996\n",
      "Step 700, Loss: -0.11551940441131592\n",
      "Step 710, Loss: -1.4115184545516968\n",
      "Step 720, Loss: -0.4807796776294708\n",
      "Step 730, Loss: 0.11248990893363953\n",
      "Step 740, Loss: 0.9327449798583984\n",
      "Step 750, Loss: -0.5837481617927551\n",
      "Step 760, Loss: -1.2403628826141357\n",
      "Step 770, Loss: 1.3741888999938965\n",
      "Step 780, Loss: 0.07746516168117523\n",
      "Checkpoint saved for epoch 1\n",
      "Epoch 2/10\n",
      "Step 0, Loss: 0.7575647830963135\n",
      "Step 10, Loss: 0.005133211612701416\n",
      "Step 20, Loss: 0.151705801486969\n",
      "Step 30, Loss: 1.1805568933486938\n",
      "Step 40, Loss: 1.619161605834961\n",
      "Step 50, Loss: -0.9467377662658691\n",
      "Step 60, Loss: -0.49442028999328613\n",
      "Step 70, Loss: 0.007304787635803223\n",
      "Step 80, Loss: -1.0021315813064575\n",
      "Step 90, Loss: -0.8854302167892456\n",
      "Step 100, Loss: 0.10501468181610107\n",
      "Step 110, Loss: -1.1024501323699951\n",
      "Step 120, Loss: 0.45800697803497314\n",
      "Step 130, Loss: -0.2572566568851471\n",
      "Step 140, Loss: 0.21916091442108154\n",
      "Step 150, Loss: 0.15004536509513855\n",
      "Step 160, Loss: -2.1215274333953857\n",
      "Step 170, Loss: -0.20256370306015015\n",
      "Step 180, Loss: 2.20672869682312\n",
      "Step 190, Loss: 1.2736736536026\n",
      "Step 200, Loss: -0.3052093982696533\n",
      "Step 210, Loss: -0.9028118252754211\n",
      "Step 220, Loss: 0.4935447573661804\n",
      "Step 230, Loss: 0.22694800794124603\n",
      "Step 240, Loss: -0.5521149039268494\n",
      "Step 250, Loss: -1.1450074911117554\n",
      "Step 260, Loss: 0.6978036165237427\n",
      "Step 270, Loss: 0.6217095851898193\n",
      "Step 280, Loss: 0.6249747276306152\n",
      "Step 290, Loss: -0.36713701486587524\n",
      "Step 300, Loss: 0.12300002574920654\n",
      "Step 310, Loss: -0.4314578175544739\n",
      "Step 320, Loss: 0.05990651249885559\n",
      "Step 330, Loss: -1.9404083490371704\n",
      "Step 340, Loss: -0.1278786063194275\n",
      "Step 350, Loss: -0.5509399771690369\n",
      "Step 360, Loss: -0.17091763019561768\n",
      "Step 370, Loss: -0.07004791498184204\n",
      "Step 380, Loss: 0.37849342823028564\n",
      "Step 390, Loss: 0.4846160411834717\n",
      "Step 400, Loss: -1.4979619979858398\n",
      "Step 410, Loss: -0.9327203631401062\n",
      "Step 420, Loss: 0.6451163291931152\n",
      "Step 430, Loss: 1.2038702964782715\n",
      "Step 440, Loss: -0.697040319442749\n",
      "Step 450, Loss: -2.0295984745025635\n",
      "Step 460, Loss: -0.44759130477905273\n",
      "Step 470, Loss: 0.903180718421936\n",
      "Step 480, Loss: 0.824715256690979\n",
      "Step 490, Loss: 1.3895573616027832\n",
      "Step 500, Loss: -1.8687323331832886\n",
      "Step 510, Loss: -1.4962677955627441\n",
      "Step 520, Loss: -0.13577497005462646\n",
      "Step 530, Loss: -0.8780213594436646\n",
      "Step 540, Loss: 0.14340580999851227\n",
      "Step 550, Loss: -0.9768033027648926\n",
      "Step 560, Loss: 0.31548070907592773\n",
      "Step 570, Loss: 0.181546151638031\n",
      "Step 580, Loss: -0.9006779193878174\n",
      "Step 590, Loss: 0.453673779964447\n",
      "Step 600, Loss: -2.3029332160949707\n",
      "Step 610, Loss: -0.027704507112503052\n",
      "Step 620, Loss: -2.1841301918029785\n",
      "Step 630, Loss: 0.6853088140487671\n",
      "Step 640, Loss: 0.26314279437065125\n",
      "Step 650, Loss: 1.5920429229736328\n",
      "Step 660, Loss: 0.9055769443511963\n",
      "Step 670, Loss: -0.8850439786911011\n",
      "Step 680, Loss: 1.033753514289856\n",
      "Step 690, Loss: 1.409797191619873\n",
      "Step 700, Loss: 1.5692965984344482\n",
      "Step 710, Loss: 0.8911848664283752\n",
      "Step 720, Loss: 1.2488573789596558\n",
      "Step 730, Loss: -0.43034639954566956\n",
      "Step 740, Loss: -1.3740479946136475\n",
      "Step 750, Loss: -1.1330047845840454\n",
      "Step 760, Loss: 1.7680654525756836\n",
      "Step 770, Loss: 1.0953032970428467\n",
      "Step 780, Loss: 1.475385308265686\n",
      "Checkpoint saved for epoch 2\n",
      "Epoch 3/10\n",
      "Step 0, Loss: 1.1110694408416748\n",
      "Step 10, Loss: -1.9918098449707031\n",
      "Step 20, Loss: 0.6902380585670471\n",
      "Step 30, Loss: -0.3714902400970459\n",
      "Step 40, Loss: 1.2098649740219116\n",
      "Step 50, Loss: -1.5968586206436157\n",
      "Step 60, Loss: -0.25894084572792053\n",
      "Step 70, Loss: -1.5743608474731445\n",
      "Step 80, Loss: 1.5689687728881836\n",
      "Step 90, Loss: -0.4199336767196655\n",
      "Step 100, Loss: 1.5286145210266113\n",
      "Step 110, Loss: -0.26388949155807495\n",
      "Step 120, Loss: 1.4229016304016113\n",
      "Step 130, Loss: -0.9727556705474854\n",
      "Step 140, Loss: -1.4817544221878052\n",
      "Step 150, Loss: -1.363476037979126\n",
      "Step 160, Loss: 2.646368980407715\n",
      "Step 170, Loss: 0.16783910989761353\n",
      "Step 180, Loss: -1.6894869804382324\n",
      "Step 190, Loss: -0.44472968578338623\n",
      "Step 200, Loss: 0.9005190134048462\n",
      "Step 210, Loss: -0.8947076797485352\n",
      "Step 220, Loss: 0.8942496180534363\n",
      "Step 230, Loss: -0.31151294708251953\n",
      "Step 240, Loss: 0.23930612206459045\n",
      "Step 250, Loss: -0.31634119153022766\n",
      "Step 260, Loss: -0.30384641885757446\n",
      "Step 270, Loss: -2.334447145462036\n",
      "Step 280, Loss: -0.33969706296920776\n",
      "Step 290, Loss: 0.5421872735023499\n",
      "Step 300, Loss: 0.0016320347785949707\n",
      "Step 310, Loss: -0.9165000915527344\n",
      "Step 320, Loss: 1.16047203540802\n",
      "Step 330, Loss: 0.8127465844154358\n",
      "Step 340, Loss: -1.1143596172332764\n",
      "Step 350, Loss: 0.47136932611465454\n",
      "Step 360, Loss: 0.6705896854400635\n",
      "Step 370, Loss: -1.1798882484436035\n",
      "Step 380, Loss: -0.5383496880531311\n",
      "Step 390, Loss: -0.24901951849460602\n",
      "Step 400, Loss: -0.18192553520202637\n",
      "Step 410, Loss: 0.8955240845680237\n",
      "Step 420, Loss: -0.30090296268463135\n",
      "Step 430, Loss: 1.7378613948822021\n",
      "Step 440, Loss: -1.436357021331787\n",
      "Step 450, Loss: 0.33368462324142456\n",
      "Step 460, Loss: 0.455684095621109\n",
      "Step 470, Loss: 0.5243774652481079\n",
      "Step 480, Loss: 1.5352145433425903\n",
      "Step 490, Loss: 2.2586143016815186\n",
      "Step 500, Loss: 2.130131244659424\n",
      "Step 510, Loss: 0.4462701678276062\n",
      "Step 520, Loss: -0.3619135618209839\n",
      "Step 530, Loss: -0.8582570552825928\n",
      "Step 540, Loss: -0.9486523866653442\n",
      "Step 550, Loss: -1.9428244829177856\n",
      "Step 560, Loss: 0.845650315284729\n",
      "Step 570, Loss: -0.011398226022720337\n",
      "Step 580, Loss: 0.4715675115585327\n",
      "Step 590, Loss: 0.8836349844932556\n",
      "Step 600, Loss: -0.05704284459352493\n",
      "Step 610, Loss: -0.5521805286407471\n",
      "Step 620, Loss: -0.5456913709640503\n",
      "Step 630, Loss: -0.6539356708526611\n",
      "Step 640, Loss: 0.08540427684783936\n",
      "Step 650, Loss: -1.9522746801376343\n",
      "Step 660, Loss: 1.5965275764465332\n",
      "Step 670, Loss: 0.9525155425071716\n",
      "Step 680, Loss: -1.2167260646820068\n",
      "Step 690, Loss: -0.321980744600296\n",
      "Step 700, Loss: 0.2674105167388916\n",
      "Step 710, Loss: 1.1368446350097656\n",
      "Step 720, Loss: 0.6248408555984497\n",
      "Step 730, Loss: -0.30470919609069824\n",
      "Step 740, Loss: 0.2652035355567932\n",
      "Step 750, Loss: 0.9305320382118225\n",
      "Step 760, Loss: 0.48019757866859436\n",
      "Step 770, Loss: 0.22199928760528564\n",
      "Step 780, Loss: -0.004692554473876953\n",
      "Checkpoint saved for epoch 3\n",
      "Epoch 4/10\n",
      "Step 0, Loss: 2.0240776538848877\n",
      "Step 10, Loss: 0.6646668910980225\n",
      "Step 20, Loss: -0.12988829612731934\n",
      "Step 30, Loss: -1.3502659797668457\n",
      "Step 40, Loss: 1.609147310256958\n",
      "Step 50, Loss: 0.010881662368774414\n",
      "Step 60, Loss: -0.3604196608066559\n",
      "Step 70, Loss: -1.442223072052002\n",
      "Step 80, Loss: 1.536224126815796\n",
      "Step 90, Loss: -0.5669906139373779\n",
      "Step 100, Loss: 1.9234423637390137\n",
      "Step 110, Loss: -1.2360974550247192\n",
      "Step 120, Loss: 0.729594349861145\n",
      "Step 130, Loss: 0.21039527654647827\n",
      "Step 140, Loss: -2.104210138320923\n",
      "Step 150, Loss: 0.23023653030395508\n",
      "Step 160, Loss: 0.16031807661056519\n",
      "Step 170, Loss: -0.5602153539657593\n",
      "Step 180, Loss: 0.8836337327957153\n",
      "Step 190, Loss: -0.6989410519599915\n",
      "Step 200, Loss: 0.7873193025588989\n",
      "Step 210, Loss: 0.16490578651428223\n",
      "Step 220, Loss: 1.1704795360565186\n",
      "Step 230, Loss: -0.9666433334350586\n",
      "Step 240, Loss: 2.223936080932617\n",
      "Step 250, Loss: 1.0141305923461914\n",
      "Step 260, Loss: 0.22772550582885742\n",
      "Step 270, Loss: 0.18500006198883057\n",
      "Step 280, Loss: 0.6327605247497559\n",
      "Step 290, Loss: 0.8086965084075928\n",
      "Step 300, Loss: 1.305312156677246\n",
      "Step 310, Loss: -0.218254953622818\n",
      "Step 320, Loss: -0.43858760595321655\n",
      "Step 330, Loss: 0.009983032941818237\n",
      "Step 340, Loss: -0.2689054012298584\n",
      "Step 350, Loss: 0.31334105134010315\n",
      "Step 360, Loss: -0.09298551082611084\n",
      "Step 370, Loss: 0.6514307856559753\n",
      "Step 380, Loss: -0.006351590156555176\n",
      "Step 390, Loss: -0.3378901481628418\n",
      "Step 400, Loss: 0.8044360280036926\n",
      "Step 410, Loss: 2.0685536861419678\n",
      "Step 420, Loss: 0.05811324715614319\n",
      "Step 430, Loss: 0.25521010160446167\n",
      "Step 440, Loss: -0.8510828614234924\n",
      "Step 450, Loss: 1.2169644832611084\n",
      "Step 460, Loss: -0.02453404664993286\n",
      "Step 470, Loss: 0.9342501163482666\n",
      "Step 480, Loss: -0.6061215400695801\n",
      "Step 490, Loss: -0.0359768271446228\n",
      "Step 500, Loss: -1.6274383068084717\n",
      "Step 510, Loss: 0.5352064371109009\n",
      "Step 520, Loss: -1.7481377124786377\n",
      "Step 530, Loss: 1.2974112033843994\n",
      "Step 540, Loss: -0.5968523025512695\n",
      "Step 550, Loss: 1.0469309091567993\n",
      "Step 560, Loss: -1.1617319583892822\n",
      "Step 570, Loss: 0.6378651857376099\n",
      "Step 580, Loss: -0.37373772263526917\n",
      "Step 590, Loss: -1.3784654140472412\n",
      "Step 600, Loss: -0.8416769504547119\n",
      "Step 610, Loss: -0.181838721036911\n",
      "Step 620, Loss: 0.14862042665481567\n",
      "Step 630, Loss: 1.727220058441162\n",
      "Step 640, Loss: 0.6659964323043823\n",
      "Step 650, Loss: -0.856089174747467\n",
      "Step 660, Loss: -0.5869108438491821\n",
      "Step 670, Loss: -0.2373543679714203\n",
      "Step 680, Loss: -0.6350405216217041\n",
      "Step 690, Loss: -0.8707201480865479\n",
      "Step 700, Loss: -1.1822773218154907\n",
      "Step 710, Loss: -1.381418228149414\n",
      "Step 720, Loss: 1.8984419107437134\n",
      "Step 730, Loss: -0.6104359030723572\n",
      "Step 740, Loss: -0.23173606395721436\n",
      "Step 750, Loss: 1.060934066772461\n",
      "Step 760, Loss: -1.4307013750076294\n",
      "Step 770, Loss: -0.7583773136138916\n",
      "Step 780, Loss: -0.07242339849472046\n",
      "Checkpoint saved for epoch 4\n",
      "Epoch 5/10\n",
      "Step 0, Loss: 0.6229074597358704\n",
      "Step 10, Loss: -1.7961421012878418\n",
      "Step 20, Loss: -0.36052006483078003\n",
      "Step 30, Loss: -1.5977731943130493\n",
      "Step 40, Loss: 0.5237458944320679\n",
      "Step 50, Loss: -0.3636663556098938\n",
      "Step 60, Loss: 0.8157702684402466\n",
      "Step 70, Loss: -0.2795953154563904\n",
      "Step 80, Loss: -1.7089020013809204\n",
      "Step 90, Loss: 1.5630481243133545\n",
      "Step 100, Loss: -2.07450532913208\n",
      "Step 110, Loss: -0.07620912790298462\n",
      "Step 120, Loss: 0.6346511244773865\n",
      "Step 130, Loss: -1.3270069360733032\n",
      "Step 140, Loss: 0.8541936874389648\n",
      "Step 150, Loss: -0.009950906038284302\n",
      "Step 160, Loss: -2.6490888595581055\n",
      "Step 170, Loss: -0.647712767124176\n",
      "Step 180, Loss: 0.7716931104660034\n",
      "Step 190, Loss: -1.3225553035736084\n",
      "Step 200, Loss: 1.5461896657943726\n",
      "Step 210, Loss: -0.04747474193572998\n",
      "Step 220, Loss: 0.2634660303592682\n",
      "Step 230, Loss: 2.3070240020751953\n",
      "Step 240, Loss: -0.7176998853683472\n",
      "Step 250, Loss: -0.6738944053649902\n",
      "Step 260, Loss: 1.3700519800186157\n",
      "Step 270, Loss: -0.6112824082374573\n",
      "Step 280, Loss: -0.4666224718093872\n",
      "Step 290, Loss: 1.282942295074463\n",
      "Step 300, Loss: 1.022096037864685\n",
      "Step 310, Loss: 2.454476833343506\n",
      "Step 320, Loss: -0.2778010070323944\n",
      "Step 330, Loss: -0.36109912395477295\n",
      "Step 340, Loss: 1.2862098217010498\n",
      "Step 350, Loss: -0.9045425057411194\n",
      "Step 360, Loss: 0.05532008409500122\n",
      "Step 370, Loss: -0.23439237475395203\n",
      "Step 380, Loss: 0.4237990975379944\n",
      "Step 390, Loss: -1.1401305198669434\n",
      "Step 400, Loss: 1.478118658065796\n",
      "Step 410, Loss: 2.0578389167785645\n",
      "Step 420, Loss: -1.8692511320114136\n",
      "Step 430, Loss: -0.011481761932373047\n",
      "Step 440, Loss: -0.8916784524917603\n",
      "Step 450, Loss: 0.54816073179245\n",
      "Step 460, Loss: -0.03371727466583252\n",
      "Step 470, Loss: 0.9440833330154419\n",
      "Step 480, Loss: -0.06468719244003296\n",
      "Step 490, Loss: -0.8723530173301697\n",
      "Step 500, Loss: -0.806320071220398\n",
      "Step 510, Loss: 2.1637673377990723\n",
      "Step 520, Loss: 0.09343260526657104\n",
      "Step 530, Loss: 0.8550910949707031\n",
      "Step 540, Loss: -2.156890869140625\n",
      "Step 550, Loss: -0.19738584756851196\n",
      "Step 560, Loss: -0.8666426539421082\n",
      "Step 570, Loss: -0.5228366851806641\n",
      "Step 580, Loss: -1.281104326248169\n",
      "Step 590, Loss: -0.34106987714767456\n",
      "Step 600, Loss: 0.7628694772720337\n",
      "Step 610, Loss: -0.4004976153373718\n",
      "Step 620, Loss: -2.527700424194336\n",
      "Step 630, Loss: 0.45744407176971436\n",
      "Step 640, Loss: -0.8618893623352051\n",
      "Step 650, Loss: -0.8040596842765808\n",
      "Step 660, Loss: 1.3271710872650146\n",
      "Step 670, Loss: -0.9658495187759399\n",
      "Step 680, Loss: 1.5006115436553955\n",
      "Step 690, Loss: 0.6503727436065674\n",
      "Step 700, Loss: 1.242288589477539\n",
      "Step 710, Loss: 0.9919083118438721\n",
      "Step 720, Loss: 0.8252343535423279\n",
      "Step 730, Loss: -0.3721548914909363\n",
      "Step 740, Loss: -0.3420757055282593\n",
      "Step 750, Loss: 0.22275327146053314\n",
      "Step 760, Loss: 0.583217978477478\n",
      "Step 770, Loss: 0.06196257472038269\n",
      "Step 780, Loss: 0.5204704999923706\n",
      "Checkpoint saved for epoch 5\n",
      "Epoch 6/10\n",
      "Step 0, Loss: 1.0537958145141602\n",
      "Step 10, Loss: 1.2215783596038818\n",
      "Step 20, Loss: 0.5330657958984375\n",
      "Step 30, Loss: 1.567406415939331\n",
      "Step 40, Loss: 2.527676820755005\n",
      "Step 50, Loss: 0.6197990775108337\n",
      "Step 60, Loss: -0.8936540484428406\n",
      "Step 70, Loss: -0.4384944438934326\n",
      "Step 80, Loss: 0.5661451816558838\n",
      "Step 90, Loss: 0.6047040224075317\n",
      "Step 100, Loss: -1.6547002792358398\n",
      "Step 110, Loss: 0.07896947860717773\n",
      "Step 120, Loss: 0.28262805938720703\n",
      "Step 130, Loss: -2.3737106323242188\n",
      "Step 140, Loss: -0.1329442262649536\n",
      "Step 150, Loss: -0.7499604225158691\n",
      "Step 160, Loss: 2.1670889854431152\n",
      "Step 170, Loss: -1.0377888679504395\n",
      "Step 180, Loss: 0.7988007068634033\n",
      "Step 190, Loss: 1.149033546447754\n",
      "Step 200, Loss: 2.2063956260681152\n",
      "Step 210, Loss: -0.8440378904342651\n",
      "Step 220, Loss: -0.766735315322876\n",
      "Step 230, Loss: -1.8385099172592163\n",
      "Step 240, Loss: 1.3193364143371582\n",
      "Step 250, Loss: 0.5382570028305054\n",
      "Step 260, Loss: 0.5506821870803833\n",
      "Step 270, Loss: 1.9878623485565186\n",
      "Step 280, Loss: 0.031429171562194824\n",
      "Step 290, Loss: -0.5812427401542664\n",
      "Step 300, Loss: -0.07385092973709106\n",
      "Step 310, Loss: 0.1934289038181305\n",
      "Step 320, Loss: -0.6952730417251587\n",
      "Step 330, Loss: 0.6828650236129761\n",
      "Step 340, Loss: -0.6636478900909424\n",
      "Step 350, Loss: -0.8275535106658936\n",
      "Step 360, Loss: -0.4099239706993103\n",
      "Step 370, Loss: 0.008633553981781006\n",
      "Step 380, Loss: 0.8118799924850464\n",
      "Step 390, Loss: -0.8407509326934814\n",
      "Step 400, Loss: 1.545384168624878\n",
      "Step 410, Loss: 0.8008389472961426\n",
      "Step 420, Loss: 1.1461107730865479\n",
      "Step 430, Loss: 1.6904622316360474\n",
      "Step 440, Loss: -0.3961564302444458\n",
      "Step 450, Loss: -0.3468242883682251\n",
      "Step 460, Loss: 0.7010824680328369\n",
      "Step 470, Loss: -0.5164691805839539\n",
      "Step 480, Loss: 1.4393670558929443\n",
      "Step 490, Loss: -0.829168438911438\n",
      "Step 500, Loss: 1.0039312839508057\n",
      "Step 510, Loss: 0.36828798055648804\n",
      "Step 520, Loss: -1.1756771802902222\n",
      "Step 530, Loss: 0.6626405119895935\n",
      "Step 540, Loss: 0.7118408679962158\n",
      "Step 550, Loss: -1.475359559059143\n",
      "Step 560, Loss: -1.0756336450576782\n",
      "Step 570, Loss: -2.1262919902801514\n",
      "Step 580, Loss: -0.15890571475028992\n",
      "Step 590, Loss: -0.47005021572113037\n",
      "Step 600, Loss: -0.3251132667064667\n",
      "Step 610, Loss: 0.7842814922332764\n",
      "Step 620, Loss: 0.6856678128242493\n",
      "Step 630, Loss: -0.7725826501846313\n",
      "Step 640, Loss: 1.7573037147521973\n",
      "Step 650, Loss: -0.5985666513442993\n",
      "Step 660, Loss: 0.373005747795105\n",
      "Step 670, Loss: -0.1437353491783142\n",
      "Step 680, Loss: -0.5994185209274292\n",
      "Step 690, Loss: 0.8137865662574768\n",
      "Step 700, Loss: -1.1574516296386719\n",
      "Step 710, Loss: -0.7785860896110535\n",
      "Step 720, Loss: 1.0400595664978027\n",
      "Step 730, Loss: -0.36804038286209106\n",
      "Step 740, Loss: -1.4410724639892578\n",
      "Step 750, Loss: -0.007228642702102661\n",
      "Step 760, Loss: -0.7387735247612\n",
      "Step 770, Loss: -0.5398592948913574\n",
      "Step 780, Loss: -0.14524811506271362\n",
      "Checkpoint saved for epoch 6\n",
      "Epoch 7/10\n",
      "Step 0, Loss: -1.529020071029663\n",
      "Step 10, Loss: 0.2983912229537964\n",
      "Step 20, Loss: 0.6095608472824097\n",
      "Step 30, Loss: -0.6503913402557373\n",
      "Step 40, Loss: -1.0288294553756714\n",
      "Step 50, Loss: 1.7900958061218262\n",
      "Step 60, Loss: -1.6242191791534424\n",
      "Step 70, Loss: 0.7450260519981384\n",
      "Step 80, Loss: 0.26511895656585693\n",
      "Step 90, Loss: -1.2026777267456055\n",
      "Step 100, Loss: 1.8153058290481567\n",
      "Step 110, Loss: 1.2587804794311523\n",
      "Step 120, Loss: 0.6943086981773376\n",
      "Step 130, Loss: 1.91249680519104\n",
      "Step 140, Loss: -0.9224093556404114\n",
      "Step 150, Loss: -0.6437926292419434\n",
      "Step 160, Loss: -0.10220110416412354\n",
      "Step 170, Loss: 0.3367392420768738\n",
      "Step 180, Loss: 0.17666570842266083\n",
      "Step 190, Loss: 0.5149444341659546\n",
      "Step 200, Loss: 0.5156366229057312\n",
      "Step 210, Loss: -0.6693264245986938\n",
      "Step 220, Loss: -1.3253962993621826\n",
      "Step 230, Loss: 1.9389735460281372\n",
      "Step 240, Loss: -0.6400625705718994\n",
      "Step 250, Loss: -0.34290188550949097\n",
      "Step 260, Loss: 1.704935073852539\n",
      "Step 270, Loss: -1.0449037551879883\n",
      "Step 280, Loss: 0.47227102518081665\n",
      "Step 290, Loss: 0.9752280712127686\n",
      "Step 300, Loss: 0.3111587166786194\n",
      "Step 310, Loss: -0.08781106770038605\n",
      "Step 320, Loss: -0.37963640689849854\n",
      "Step 330, Loss: 2.1361818313598633\n",
      "Step 340, Loss: -0.4668055474758148\n",
      "Step 350, Loss: 1.010844349861145\n",
      "Step 360, Loss: 0.9216016530990601\n",
      "Step 370, Loss: 1.227668285369873\n",
      "Step 380, Loss: 0.12282103300094604\n",
      "Step 390, Loss: -0.3085898458957672\n",
      "Step 400, Loss: 0.5457647442817688\n",
      "Step 410, Loss: 0.5270082950592041\n",
      "Step 420, Loss: 0.9150841236114502\n",
      "Step 430, Loss: -1.2649364471435547\n",
      "Step 440, Loss: -0.6795449256896973\n",
      "Step 450, Loss: 0.5417468547821045\n",
      "Step 460, Loss: 0.4144459664821625\n",
      "Step 470, Loss: -1.415223240852356\n",
      "Step 480, Loss: 0.5664530992507935\n",
      "Step 490, Loss: 0.624631404876709\n",
      "Step 500, Loss: 0.49307364225387573\n",
      "Step 510, Loss: -0.8012348413467407\n",
      "Step 520, Loss: 0.8230565190315247\n",
      "Step 530, Loss: -1.2436773777008057\n",
      "Step 540, Loss: 1.123031735420227\n",
      "Step 550, Loss: -0.010456770658493042\n",
      "Step 560, Loss: -0.6613303422927856\n",
      "Step 570, Loss: 1.508458137512207\n",
      "Step 580, Loss: 0.6671109795570374\n",
      "Step 590, Loss: -0.07055693864822388\n",
      "Step 600, Loss: -2.7114036083221436\n",
      "Step 610, Loss: -1.259887933731079\n",
      "Step 620, Loss: -0.3519303500652313\n",
      "Step 630, Loss: 0.7725526094436646\n",
      "Step 640, Loss: 0.1660081148147583\n",
      "Step 650, Loss: 2.139660596847534\n",
      "Step 660, Loss: -1.226933240890503\n",
      "Step 670, Loss: 0.951331377029419\n",
      "Step 680, Loss: 0.42413216829299927\n",
      "Step 690, Loss: -1.8998934030532837\n",
      "Step 700, Loss: -0.5363590121269226\n",
      "Step 710, Loss: -0.1153542697429657\n",
      "Step 720, Loss: -0.053393423557281494\n",
      "Step 730, Loss: 0.5929442048072815\n",
      "Step 740, Loss: -0.8477243185043335\n",
      "Step 750, Loss: -1.1839280128479004\n",
      "Step 760, Loss: 0.13202422857284546\n",
      "Step 770, Loss: -0.5995098948478699\n",
      "Step 780, Loss: 0.8953092098236084\n",
      "Checkpoint saved for epoch 7\n",
      "Epoch 8/10\n",
      "Step 0, Loss: 1.3075551986694336\n",
      "Step 10, Loss: 0.04678410291671753\n",
      "Step 20, Loss: -2.1429543495178223\n",
      "Step 30, Loss: 1.113189935684204\n",
      "Step 40, Loss: -1.6916606426239014\n",
      "Step 50, Loss: -1.063515305519104\n",
      "Step 60, Loss: 1.0830589532852173\n",
      "Step 70, Loss: -0.8110654354095459\n",
      "Step 80, Loss: -1.444742202758789\n",
      "Step 90, Loss: 0.48537322878837585\n",
      "Step 100, Loss: -0.6289513111114502\n",
      "Step 110, Loss: -0.18279129266738892\n",
      "Step 120, Loss: -0.3276330232620239\n",
      "Step 130, Loss: 0.9329584240913391\n",
      "Step 140, Loss: 1.1804487705230713\n",
      "Step 150, Loss: 1.3799268007278442\n",
      "Step 160, Loss: -1.2584013938903809\n",
      "Step 170, Loss: 0.7564529776573181\n",
      "Step 180, Loss: -0.29513293504714966\n",
      "Step 190, Loss: 0.5194002389907837\n",
      "Step 200, Loss: -0.45841479301452637\n",
      "Step 210, Loss: 1.9745144844055176\n",
      "Step 220, Loss: -1.8273565769195557\n",
      "Step 230, Loss: 1.14058518409729\n",
      "Step 240, Loss: 1.254533290863037\n",
      "Step 250, Loss: 0.7306705117225647\n",
      "Step 260, Loss: -0.019446969032287598\n",
      "Step 270, Loss: -1.368882656097412\n",
      "Step 280, Loss: -0.6576554179191589\n",
      "Step 290, Loss: -0.9133312702178955\n",
      "Step 300, Loss: 0.8452717065811157\n",
      "Step 310, Loss: 1.2990195751190186\n",
      "Step 320, Loss: 0.8316464424133301\n",
      "Step 330, Loss: -0.10903236269950867\n",
      "Step 340, Loss: -0.5551239848136902\n",
      "Step 350, Loss: -1.4220240116119385\n",
      "Step 360, Loss: -2.176363706588745\n",
      "Step 370, Loss: 2.0689895153045654\n",
      "Step 380, Loss: 1.529754400253296\n",
      "Step 390, Loss: 2.2724528312683105\n",
      "Step 400, Loss: -0.05152249336242676\n",
      "Step 410, Loss: -1.4195433855056763\n",
      "Step 420, Loss: -0.25374147295951843\n",
      "Step 430, Loss: -0.9626794457435608\n",
      "Step 440, Loss: -1.4383677244186401\n",
      "Step 450, Loss: -1.519834041595459\n",
      "Step 460, Loss: 2.6191036701202393\n",
      "Step 470, Loss: 0.8826261162757874\n",
      "Step 480, Loss: 0.062145888805389404\n",
      "Step 490, Loss: 0.043885186314582825\n",
      "Step 500, Loss: 0.5185539722442627\n",
      "Step 510, Loss: 1.0032700300216675\n",
      "Step 520, Loss: 0.5508075952529907\n",
      "Step 530, Loss: -1.1279135942459106\n",
      "Step 540, Loss: -0.2645878791809082\n",
      "Step 550, Loss: -1.2733445167541504\n",
      "Step 560, Loss: -0.49032294750213623\n",
      "Step 570, Loss: -0.7486863136291504\n",
      "Step 580, Loss: -1.1924264430999756\n",
      "Step 590, Loss: -0.05103802680969238\n",
      "Step 600, Loss: 1.8155437707901\n",
      "Step 610, Loss: 1.0572106838226318\n",
      "Step 620, Loss: -1.3033206462860107\n",
      "Step 630, Loss: -0.9907393455505371\n",
      "Step 640, Loss: 0.058126986026763916\n",
      "Step 650, Loss: -0.8081532120704651\n",
      "Step 660, Loss: 1.1639952659606934\n",
      "Step 670, Loss: 1.40701425075531\n",
      "Step 680, Loss: -1.4837499856948853\n",
      "Step 690, Loss: 1.4780247211456299\n",
      "Step 700, Loss: -0.3462398052215576\n",
      "Step 710, Loss: 0.17973536252975464\n",
      "Step 720, Loss: -0.9384194612503052\n",
      "Step 730, Loss: -1.1931551694869995\n",
      "Step 740, Loss: -1.054555892944336\n",
      "Step 750, Loss: -1.5812593698501587\n",
      "Step 760, Loss: 0.28859102725982666\n",
      "Step 770, Loss: 0.029879719018936157\n",
      "Step 780, Loss: -1.5231274366378784\n",
      "Checkpoint saved for epoch 8\n",
      "Epoch 9/10\n",
      "Step 0, Loss: 1.406074047088623\n",
      "Step 10, Loss: 1.5530261993408203\n",
      "Step 20, Loss: 0.559151291847229\n",
      "Step 30, Loss: 0.07870256900787354\n",
      "Step 40, Loss: 1.6199835538864136\n",
      "Step 50, Loss: -0.26828932762145996\n",
      "Step 60, Loss: 0.7692727446556091\n",
      "Step 70, Loss: 0.0827658474445343\n",
      "Step 80, Loss: 0.19739767909049988\n",
      "Step 90, Loss: -1.2355707883834839\n",
      "Step 100, Loss: 0.6170476675033569\n",
      "Step 110, Loss: -1.5244005918502808\n",
      "Step 120, Loss: 0.09328904747962952\n",
      "Step 130, Loss: 0.8786423206329346\n",
      "Step 140, Loss: -0.851941704750061\n",
      "Step 150, Loss: -0.18019801378250122\n",
      "Step 160, Loss: 1.061708927154541\n",
      "Step 170, Loss: 1.2909128665924072\n",
      "Step 180, Loss: 2.203579902648926\n",
      "Step 190, Loss: -1.0699963569641113\n",
      "Step 200, Loss: 0.10180298984050751\n",
      "Step 210, Loss: -1.2105517387390137\n",
      "Step 220, Loss: 1.625622034072876\n",
      "Step 230, Loss: 0.4789200723171234\n",
      "Step 240, Loss: 0.10738836228847504\n",
      "Step 250, Loss: 0.14610306918621063\n",
      "Step 260, Loss: -1.5429408550262451\n",
      "Step 270, Loss: -0.9692180156707764\n",
      "Step 280, Loss: 0.1623486578464508\n",
      "Step 290, Loss: 1.538689136505127\n",
      "Step 300, Loss: -0.08492037653923035\n",
      "Step 310, Loss: -0.0005258917808532715\n",
      "Step 320, Loss: -0.8778539299964905\n",
      "Step 330, Loss: -0.08502127230167389\n",
      "Step 340, Loss: -0.8327816724777222\n",
      "Step 350, Loss: -0.6082097291946411\n",
      "Step 360, Loss: 0.16008740663528442\n",
      "Step 370, Loss: 1.062056064605713\n",
      "Step 380, Loss: -1.193936824798584\n",
      "Step 390, Loss: 0.6225324869155884\n",
      "Step 400, Loss: 0.6628243327140808\n",
      "Step 410, Loss: -1.4407986402511597\n",
      "Step 420, Loss: -0.7522718906402588\n",
      "Step 430, Loss: -0.006668567657470703\n",
      "Step 440, Loss: 0.7027986645698547\n",
      "Step 450, Loss: 1.5449923276901245\n",
      "Step 460, Loss: -1.1596699953079224\n",
      "Step 470, Loss: -0.299419105052948\n",
      "Step 480, Loss: -0.9285505414009094\n",
      "Step 490, Loss: 0.513952910900116\n",
      "Step 500, Loss: -0.022992312908172607\n",
      "Step 510, Loss: 1.653157353401184\n",
      "Step 520, Loss: 0.15398147702217102\n",
      "Step 530, Loss: 0.6214057207107544\n",
      "Step 540, Loss: 1.2691096067428589\n",
      "Step 550, Loss: -0.9623799324035645\n",
      "Step 560, Loss: -0.6807887554168701\n",
      "Step 570, Loss: 0.7157918214797974\n",
      "Step 580, Loss: 0.9523910284042358\n",
      "Step 590, Loss: 0.11000335216522217\n",
      "Step 600, Loss: -0.5983459949493408\n",
      "Step 610, Loss: -0.34561687707901\n",
      "Step 620, Loss: -0.429551899433136\n",
      "Step 630, Loss: -0.46933409571647644\n",
      "Step 640, Loss: -1.3297642469406128\n",
      "Step 650, Loss: 0.8310502171516418\n",
      "Step 660, Loss: -0.2474173903465271\n",
      "Step 670, Loss: -0.23726209998130798\n",
      "Step 680, Loss: -0.47905972599983215\n",
      "Step 690, Loss: 0.8939692974090576\n",
      "Step 700, Loss: -0.23007440567016602\n",
      "Step 710, Loss: -0.745391845703125\n",
      "Step 720, Loss: 0.8022918701171875\n",
      "Step 730, Loss: -0.9346688389778137\n",
      "Step 740, Loss: 0.6140421032905579\n",
      "Step 750, Loss: -0.7862017154693604\n",
      "Step 760, Loss: 0.32932519912719727\n",
      "Step 770, Loss: -1.6791107654571533\n",
      "Step 780, Loss: -1.2082386016845703\n",
      "Checkpoint saved for epoch 9\n",
      "Epoch 10/10\n",
      "Step 0, Loss: 0.29221269488334656\n",
      "Step 10, Loss: 0.30148807168006897\n",
      "Step 20, Loss: -0.4031031131744385\n",
      "Step 30, Loss: -0.3858655095100403\n",
      "Step 40, Loss: -1.2118133306503296\n",
      "Step 50, Loss: 0.9746559858322144\n",
      "Step 60, Loss: 0.36642372608184814\n",
      "Step 70, Loss: 1.6500649452209473\n",
      "Step 80, Loss: 1.0695828199386597\n",
      "Step 90, Loss: -1.3476836681365967\n",
      "Step 100, Loss: -0.6841040253639221\n",
      "Step 110, Loss: 0.8443665504455566\n",
      "Step 120, Loss: 1.94637131690979\n",
      "Step 130, Loss: 0.7035589218139648\n",
      "Step 140, Loss: 0.3092067837715149\n",
      "Step 150, Loss: 0.3354296088218689\n",
      "Step 160, Loss: -0.6707030534744263\n",
      "Step 170, Loss: -0.07860234379768372\n",
      "Step 180, Loss: -0.3669763505458832\n",
      "Step 190, Loss: 0.21536767482757568\n",
      "Step 200, Loss: -1.5122402906417847\n",
      "Step 210, Loss: -0.3054625391960144\n",
      "Step 220, Loss: 1.8590195178985596\n",
      "Step 230, Loss: 0.11188548803329468\n",
      "Step 240, Loss: 1.7382159233093262\n",
      "Step 250, Loss: 0.648390531539917\n",
      "Step 260, Loss: 1.3831477165222168\n",
      "Step 270, Loss: 0.4885554015636444\n",
      "Step 280, Loss: 0.8665914535522461\n",
      "Step 290, Loss: 1.0198450088500977\n",
      "Step 300, Loss: -0.21246913075447083\n",
      "Step 310, Loss: 0.3893583416938782\n",
      "Step 320, Loss: 1.5252991914749146\n",
      "Step 330, Loss: 1.1399273872375488\n",
      "Step 340, Loss: 0.993323564529419\n",
      "Step 350, Loss: -0.6089104413986206\n",
      "Step 360, Loss: -0.09700685739517212\n",
      "Step 370, Loss: -0.5160005688667297\n",
      "Step 380, Loss: -1.2466228008270264\n",
      "Step 390, Loss: 0.5568363666534424\n",
      "Step 400, Loss: 0.7418290376663208\n",
      "Step 410, Loss: -0.14423930644989014\n",
      "Step 420, Loss: 1.0541095733642578\n",
      "Step 430, Loss: 0.3959820866584778\n",
      "Step 440, Loss: -1.5281214714050293\n",
      "Step 450, Loss: -0.011026859283447266\n",
      "Step 460, Loss: -1.0577113628387451\n",
      "Step 470, Loss: 0.5648428201675415\n",
      "Step 480, Loss: -1.6001875400543213\n",
      "Step 490, Loss: -1.2557048797607422\n",
      "Step 500, Loss: 0.3798455595970154\n",
      "Step 510, Loss: -0.45654749870300293\n",
      "Step 520, Loss: 1.3989877700805664\n",
      "Step 530, Loss: 1.1834522485733032\n",
      "Step 540, Loss: -0.4430232644081116\n",
      "Step 550, Loss: -0.7786881923675537\n",
      "Step 560, Loss: -1.3836801052093506\n",
      "Step 570, Loss: 0.7251468896865845\n",
      "Step 580, Loss: -1.0203276872634888\n",
      "Step 590, Loss: 0.5354911088943481\n",
      "Step 600, Loss: -0.7066731452941895\n",
      "Step 610, Loss: -1.4983960390090942\n",
      "Step 620, Loss: -0.519364058971405\n",
      "Step 630, Loss: -1.049269199371338\n",
      "Step 640, Loss: 0.46849918365478516\n",
      "Step 650, Loss: -0.4813707172870636\n",
      "Step 660, Loss: 0.948038637638092\n",
      "Step 670, Loss: 0.7613375782966614\n",
      "Step 680, Loss: 0.3425273597240448\n",
      "Step 690, Loss: 1.5552496910095215\n",
      "Step 700, Loss: -0.1079622209072113\n",
      "Step 710, Loss: -0.15989339351654053\n",
      "Step 720, Loss: -0.9218989610671997\n",
      "Step 730, Loss: -0.5859143733978271\n",
      "Step 740, Loss: -0.00945688784122467\n",
      "Step 750, Loss: -0.2665412425994873\n",
      "Step 760, Loss: 0.5176054239273071\n",
      "Step 770, Loss: 0.4156847894191742\n",
      "Step 780, Loss: -0.4918222725391388\n",
      "Checkpoint saved for epoch 10\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths to label files\n",
    "    cifar10_label_file = \"./data/CIFAR-10_human_ordered.npy\"\n",
    "    cifar100_label_file = \"./data/CIFAR-100_human_ordered.npy\"\n",
    "\n",
    "    # Load CIFAR-10 dataset for training\n",
    "    train_ds_cifar10 = load_combined_dataset(\"cifar10\", cifar10_label_file, num_classes=10)\n",
    "\n",
    "    # Train on CIFAR-10 (CIFAR-100 can be loaded similarly)\n",
    "    train_model(train_ds_cifar10, epochs=10, num_classes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
