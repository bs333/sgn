{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-ratio transformations\n",
    "def clr_inv(p):\n",
    "    z = tf.math.log(p)\n",
    "    return z - tf.reduce_mean(z, axis=1)[:, tf.newaxis]\n",
    "\n",
    "def clr_forward(z, axis=1):\n",
    "    return tf.nn.softmax(z, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_resnet(num_classes, depth=28, width=2):\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "    for _ in range(depth // 6):\n",
    "        x = layers.Conv2D(16 * width, (3, 3), padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)  # Use Keras's ReLU layer\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader combining TF CIFAR datasets with custom labels\n",
    "def load_combined_dataset(cifar_version, label_file, num_classes):\n",
    "    \"\"\"\n",
    "    Combine CIFAR image data from TensorFlow with labels from a .npy file.\n",
    "\n",
    "    Args:\n",
    "        cifar_version (str): \"cifar10\" or \"cifar100\" to select the dataset.\n",
    "        label_file (str): Path to the .npy file containing labels.\n",
    "        num_classes (int): Number of classes (10 or 100).\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset ready for training/testing.\n",
    "    \"\"\"\n",
    "    # Load image data\n",
    "    if cifar_version == \"cifar10\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar10.load_data()\n",
    "    elif cifar_version == \"cifar100\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar100.load_data()\n",
    "    else:\n",
    "        raise ValueError(\"cifar_version must be 'cifar10' or 'cifar100'.\")\n",
    "\n",
    "    # Load labels from .npy file\n",
    "    label_data = np.load(label_file, allow_pickle=True).item()\n",
    "    if 'clean_label' not in label_data:\n",
    "        raise ValueError(\"Label file must contain 'clean_label' key.\")\n",
    "\n",
    "    # Normalize image data and convert labels\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    y_train = tf.keras.utils.to_categorical(label_data['clean_label'], num_classes)\n",
    "\n",
    "    # Create TensorFlow dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian noise (manual implementation)\n",
    "def add_gaussian_noise(inputs, mean=0.0, stddev=0.1):\n",
    "    noise = tf.random.normal(shape=tf.shape(inputs), mean=mean, stddev=stddev)\n",
    "    return inputs + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(model, images, labels, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        smoothed_targets = clr_forward(labels)\n",
    "        logit_targets = clr_inv(smoothed_targets)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(logit_targets, logits))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train_model(train_ds, epochs=10, num_classes=10):\n",
    "    model = wide_resnet(num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # Define metrics\n",
    "    accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    precision_metric = tf.keras.metrics.Precision()\n",
    "    recall_metric = tf.keras.metrics.Recall()\n",
    "\n",
    "    # DataFrame to store metrics\n",
    "    metrics_df = pd.DataFrame(columns=[\"Epoch\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        epoch_loss_sum = 0\n",
    "        num_steps = 0\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        accuracy_metric.reset_state()\n",
    "        precision_metric.reset_state()\n",
    "        recall_metric.reset_state()\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_ds):\n",
    "            # Perform training step\n",
    "            loss = train_step(model, images, labels, optimizer)\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss_sum += loss.numpy()\n",
    "            num_steps += 1\n",
    "            predictions = model(images, training=False)\n",
    "            accuracy_metric.update_state(labels, predictions)\n",
    "            precision_metric.update_state(labels, predictions)\n",
    "            recall_metric.update_state(labels, predictions)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        # Compute metrics for the epoch\n",
    "        avg_loss = epoch_loss_sum / num_steps\n",
    "        epoch_accuracy = accuracy_metric.result().numpy()\n",
    "        epoch_precision = precision_metric.result().numpy()\n",
    "        epoch_recall = recall_metric.result().numpy()\n",
    "        epoch_f1_score = 2 * (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall + 1e-7)\n",
    "\n",
    "        # Display metrics\n",
    "        print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}, \"\n",
    "              f\"Accuracy: {epoch_accuracy:.4f}, \"\n",
    "              f\"Precision: {epoch_precision:.4f}, \"\n",
    "              f\"Recall: {epoch_recall:.4f}, \"\n",
    "              f\"F1-Score: {epoch_f1_score:.4f}\")\n",
    "\n",
    "        # Save metrics to DataFrame\n",
    "        metrics_df = pd.concat([\n",
    "            metrics_df,\n",
    "            pd.DataFrame([{\n",
    "                \"Epoch\": epoch + 1,\n",
    "                \"Loss\": avg_loss,\n",
    "                \"Accuracy\": epoch_accuracy,\n",
    "                \"Precision\": epoch_precision,\n",
    "                \"Recall\": epoch_recall,\n",
    "                \"F1-Score\": epoch_f1_score\n",
    "            }])\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        # Save checkpoint with proper extension\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        model.save_weights(f\"checkpoints/sgn_epoch_{epoch}.weights.h5\")\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n",
    "\n",
    "    # Save metrics to a CSV file\n",
    "    metrics_df.to_csv(\"training_metrics.csv\", index=False)\n",
    "    print(\"Metrics saved to 'training_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Step 0, Loss: 1.6486167907714844\n",
      "Step 10, Loss: 0.4435200095176697\n",
      "Step 20, Loss: -1.1426292657852173\n",
      "Step 30, Loss: -0.3364371657371521\n",
      "Step 40, Loss: -0.4337804317474365\n",
      "Step 50, Loss: -0.19734880328178406\n",
      "Step 60, Loss: 2.5535049438476562\n",
      "Step 70, Loss: 0.7717896103858948\n",
      "Step 80, Loss: 0.411693811416626\n",
      "Step 90, Loss: -0.1919950395822525\n",
      "Step 100, Loss: 0.1359441876411438\n",
      "Step 110, Loss: -1.8039202690124512\n",
      "Step 120, Loss: 1.0233824253082275\n",
      "Step 130, Loss: 0.4591432213783264\n",
      "Step 140, Loss: -0.15268312394618988\n",
      "Step 150, Loss: 0.6802430748939514\n",
      "Step 160, Loss: 0.05132439732551575\n",
      "Step 170, Loss: 0.2686876654624939\n",
      "Step 180, Loss: 1.0770695209503174\n",
      "Step 190, Loss: 0.7185894846916199\n",
      "Step 200, Loss: 1.5800483226776123\n",
      "Step 210, Loss: 0.10330504179000854\n",
      "Step 220, Loss: -3.2299559116363525\n",
      "Step 230, Loss: 0.8274235725402832\n",
      "Step 240, Loss: 0.21352875232696533\n",
      "Step 250, Loss: -0.8823997378349304\n",
      "Step 260, Loss: 0.22993344068527222\n",
      "Step 270, Loss: 0.5585253238677979\n",
      "Step 280, Loss: 1.2836320400238037\n",
      "Step 290, Loss: 0.01777222752571106\n",
      "Step 300, Loss: -0.6863881349563599\n",
      "Step 310, Loss: -0.20056742429733276\n",
      "Step 320, Loss: 0.18870773911476135\n",
      "Step 330, Loss: 0.8498045206069946\n",
      "Step 340, Loss: -0.9178277254104614\n",
      "Step 350, Loss: 1.5264384746551514\n",
      "Step 360, Loss: -1.3522449731826782\n",
      "Step 370, Loss: -0.4446273446083069\n",
      "Step 380, Loss: 0.7929771542549133\n",
      "Step 390, Loss: -1.7172024250030518\n",
      "Step 400, Loss: -1.069278597831726\n",
      "Step 410, Loss: 0.8163408637046814\n",
      "Step 420, Loss: 1.1213202476501465\n",
      "Step 430, Loss: 1.2501323223114014\n",
      "Step 440, Loss: -0.982581377029419\n",
      "Step 450, Loss: -0.8590899705886841\n",
      "Step 460, Loss: 1.6322306394577026\n",
      "Step 470, Loss: 1.4737186431884766\n",
      "Step 480, Loss: 0.7457826137542725\n",
      "Step 490, Loss: -0.19975242018699646\n",
      "Step 500, Loss: -0.5184316635131836\n",
      "Step 510, Loss: -0.8136034607887268\n",
      "Step 520, Loss: 1.7051401138305664\n",
      "Step 530, Loss: 2.0453715324401855\n",
      "Step 540, Loss: 0.8836873769760132\n",
      "Step 550, Loss: 0.6212493777275085\n",
      "Step 560, Loss: -0.5961886644363403\n",
      "Step 570, Loss: 0.12758994102478027\n",
      "Step 580, Loss: 0.3345364034175873\n",
      "Step 590, Loss: -0.11618763208389282\n",
      "Step 600, Loss: -0.5609025359153748\n",
      "Step 610, Loss: 0.7507820129394531\n",
      "Step 620, Loss: 0.2258796989917755\n",
      "Step 630, Loss: -0.5152218341827393\n",
      "Step 640, Loss: -2.354031801223755\n",
      "Step 650, Loss: 0.7896015644073486\n",
      "Step 660, Loss: -0.8406053781509399\n",
      "Step 670, Loss: -1.174159288406372\n",
      "Step 680, Loss: 0.5480325222015381\n",
      "Step 690, Loss: -0.8679875731468201\n",
      "Step 700, Loss: -0.2695419490337372\n",
      "Step 710, Loss: 1.0874768495559692\n",
      "Step 720, Loss: -0.30358660221099854\n",
      "Step 730, Loss: 0.5042787790298462\n",
      "Step 740, Loss: 0.8152884244918823\n",
      "Step 750, Loss: -0.3235912024974823\n",
      "Step 760, Loss: 2.0014028549194336\n",
      "Step 770, Loss: 0.25541752576828003\n",
      "Step 780, Loss: -0.6379300355911255\n",
      "Epoch 1 - Average Loss: -0.0096, Accuracy: 0.0988, Precision: 0.1000, Recall: 0.2361, F1-Score: 0.1405\n",
      "Checkpoint saved for epoch 1\n",
      "\n",
      "Epoch 2/10\n",
      "Step 0, Loss: 0.07644703984260559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbhatia2\\AppData\\Local\\Temp\\ipykernel_33536\\982803084.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, Loss: -0.8859335780143738\n",
      "Step 20, Loss: -1.0243818759918213\n",
      "Step 30, Loss: 0.46508583426475525\n",
      "Step 40, Loss: 0.8832007646560669\n",
      "Step 50, Loss: -2.257573366165161\n",
      "Step 60, Loss: -0.12125816941261292\n",
      "Step 70, Loss: 0.00027620792388916016\n",
      "Step 80, Loss: 0.3085614740848541\n",
      "Step 90, Loss: -0.41359877586364746\n",
      "Step 100, Loss: 0.16238999366760254\n",
      "Step 110, Loss: -1.3019776344299316\n",
      "Step 120, Loss: -0.5895940661430359\n",
      "Step 130, Loss: 2.0273990631103516\n",
      "Step 140, Loss: -0.0037988126277923584\n",
      "Step 150, Loss: -0.30946171283721924\n",
      "Step 160, Loss: 0.42292946577072144\n",
      "Step 170, Loss: 0.4401721954345703\n",
      "Step 180, Loss: -0.16463184356689453\n",
      "Step 190, Loss: -0.4752507209777832\n",
      "Step 200, Loss: 0.3126112222671509\n",
      "Step 210, Loss: 1.0764703750610352\n",
      "Step 220, Loss: 2.3178954124450684\n",
      "Step 230, Loss: -0.12880921363830566\n",
      "Step 240, Loss: 1.8546595573425293\n",
      "Step 250, Loss: 0.38878583908081055\n",
      "Step 260, Loss: -1.050205945968628\n",
      "Step 270, Loss: 0.33870929479599\n",
      "Step 280, Loss: -0.688868522644043\n",
      "Step 290, Loss: -2.792113780975342\n",
      "Step 300, Loss: 0.6888007521629333\n",
      "Step 310, Loss: 1.1512490510940552\n",
      "Step 320, Loss: 0.006484478712081909\n",
      "Step 330, Loss: -2.0407910346984863\n",
      "Step 340, Loss: 0.2048981785774231\n",
      "Step 350, Loss: 0.30240294337272644\n",
      "Step 360, Loss: 0.9139516353607178\n",
      "Step 370, Loss: 1.4192538261413574\n",
      "Step 380, Loss: -0.35753682255744934\n",
      "Step 390, Loss: -0.11726126074790955\n",
      "Step 400, Loss: 0.32784557342529297\n",
      "Step 410, Loss: -1.5851492881774902\n",
      "Step 420, Loss: 0.9530441164970398\n",
      "Step 430, Loss: 1.2273852825164795\n",
      "Step 440, Loss: 0.9735410809516907\n",
      "Step 450, Loss: -0.5860450863838196\n",
      "Step 460, Loss: -0.19755785167217255\n",
      "Step 470, Loss: -0.019865453243255615\n",
      "Step 480, Loss: -0.42507684230804443\n",
      "Step 490, Loss: 0.6467846632003784\n",
      "Step 500, Loss: -0.9401972889900208\n",
      "Step 510, Loss: 1.135329246520996\n",
      "Step 520, Loss: -0.21814200282096863\n",
      "Step 530, Loss: -0.3882278800010681\n",
      "Step 540, Loss: -0.5574105978012085\n",
      "Step 550, Loss: -0.032751619815826416\n",
      "Step 560, Loss: -0.0030084848403930664\n",
      "Step 570, Loss: 1.1693601608276367\n",
      "Step 580, Loss: 0.5941168069839478\n",
      "Step 590, Loss: 0.3346738815307617\n",
      "Step 600, Loss: -0.339306116104126\n",
      "Step 610, Loss: -0.6211840510368347\n",
      "Step 620, Loss: 1.3614531755447388\n",
      "Step 630, Loss: -0.26604539155960083\n",
      "Step 640, Loss: 0.9565756320953369\n",
      "Step 650, Loss: -0.045673027634620667\n",
      "Step 660, Loss: 0.5945459008216858\n",
      "Step 670, Loss: 0.6610349416732788\n",
      "Step 680, Loss: -1.4464370012283325\n",
      "Step 690, Loss: -0.8476189374923706\n",
      "Step 700, Loss: 0.31928619742393494\n",
      "Step 710, Loss: 0.15578095614910126\n",
      "Step 720, Loss: 0.6031652688980103\n",
      "Step 730, Loss: -1.2545490264892578\n",
      "Step 740, Loss: -1.2923327684402466\n",
      "Step 750, Loss: 1.1857945919036865\n",
      "Step 760, Loss: -0.601639986038208\n",
      "Step 770, Loss: 1.6899468898773193\n",
      "Step 780, Loss: -0.9123612642288208\n",
      "Epoch 2 - Average Loss: -0.0052, Accuracy: 0.0982, Precision: 0.0999, Recall: 0.3694, F1-Score: 0.1572\n",
      "Checkpoint saved for epoch 2\n",
      "\n",
      "Epoch 3/10\n",
      "Step 0, Loss: -1.1073309183120728\n",
      "Step 10, Loss: -0.5825997591018677\n",
      "Step 20, Loss: -0.02931705117225647\n",
      "Step 30, Loss: -0.9075274467468262\n",
      "Step 40, Loss: -2.406609058380127\n",
      "Step 50, Loss: -0.26491203904151917\n",
      "Step 60, Loss: -0.15192878246307373\n",
      "Step 70, Loss: 1.3543801307678223\n",
      "Step 80, Loss: -1.047300100326538\n",
      "Step 90, Loss: 1.152351975440979\n",
      "Step 100, Loss: -0.17333954572677612\n",
      "Step 110, Loss: -2.0280704498291016\n",
      "Step 120, Loss: -0.24782434105873108\n",
      "Step 130, Loss: 0.8801504373550415\n",
      "Step 140, Loss: 0.6040544509887695\n",
      "Step 150, Loss: -0.5787146091461182\n",
      "Step 160, Loss: -1.0041357278823853\n",
      "Step 170, Loss: -0.6240538358688354\n",
      "Step 180, Loss: 1.1064324378967285\n",
      "Step 190, Loss: -0.20850405097007751\n",
      "Step 200, Loss: -0.4922104775905609\n",
      "Step 210, Loss: 1.8686676025390625\n",
      "Step 220, Loss: -1.308868169784546\n",
      "Step 230, Loss: -1.0634605884552002\n",
      "Step 240, Loss: 0.6602267026901245\n",
      "Step 250, Loss: -0.17126794159412384\n",
      "Step 260, Loss: -1.5822889804840088\n",
      "Step 270, Loss: 0.6787101626396179\n",
      "Step 280, Loss: 1.0543426275253296\n",
      "Step 290, Loss: 1.0059751272201538\n",
      "Step 300, Loss: -2.076899766921997\n",
      "Step 310, Loss: 0.18739745020866394\n",
      "Step 320, Loss: 2.2340874671936035\n",
      "Step 330, Loss: -0.21868610382080078\n",
      "Step 340, Loss: -0.06760168075561523\n",
      "Step 350, Loss: -0.19332575798034668\n",
      "Step 360, Loss: 0.20772437751293182\n",
      "Step 370, Loss: -0.3140092194080353\n",
      "Step 380, Loss: -0.2846347689628601\n",
      "Step 390, Loss: 0.34146010875701904\n",
      "Step 400, Loss: -0.2761216163635254\n",
      "Step 410, Loss: -1.3411856889724731\n",
      "Step 420, Loss: -1.2750602960586548\n",
      "Step 430, Loss: -0.6266674995422363\n",
      "Step 440, Loss: -0.2368798851966858\n",
      "Step 450, Loss: 1.5008025169372559\n",
      "Step 460, Loss: -0.37051528692245483\n",
      "Step 470, Loss: -1.2054688930511475\n",
      "Step 480, Loss: -1.15125572681427\n",
      "Step 490, Loss: -1.7626196146011353\n",
      "Step 500, Loss: 1.3793014287948608\n",
      "Step 510, Loss: -0.3620591461658478\n",
      "Step 520, Loss: 0.8263529539108276\n",
      "Step 530, Loss: -0.31845372915267944\n",
      "Step 540, Loss: 0.1492653489112854\n",
      "Step 550, Loss: -1.4091964960098267\n",
      "Step 560, Loss: 0.19975359737873077\n",
      "Step 570, Loss: 2.1266822814941406\n",
      "Step 580, Loss: 1.3006123304367065\n",
      "Step 590, Loss: 1.0987834930419922\n",
      "Step 600, Loss: -0.9071650505065918\n",
      "Step 610, Loss: -1.2924060821533203\n",
      "Step 620, Loss: 1.1447629928588867\n",
      "Step 630, Loss: 0.20961284637451172\n",
      "Step 640, Loss: -0.27188169956207275\n",
      "Step 650, Loss: -0.08059766888618469\n",
      "Step 660, Loss: -0.13676297664642334\n",
      "Step 670, Loss: -1.0087133646011353\n",
      "Step 680, Loss: 0.24531781673431396\n",
      "Step 690, Loss: 0.5804555416107178\n",
      "Step 700, Loss: 0.9536606073379517\n",
      "Step 710, Loss: 0.2555009126663208\n",
      "Step 720, Loss: 0.4605748653411865\n",
      "Step 730, Loss: 1.9894726276397705\n",
      "Step 740, Loss: 2.1505699157714844\n",
      "Step 750, Loss: 1.7558835744857788\n",
      "Step 760, Loss: -0.5905658602714539\n",
      "Step 770, Loss: 0.3819536566734314\n",
      "Step 780, Loss: 0.2800632119178772\n",
      "Epoch 3 - Average Loss: 0.0084, Accuracy: 0.0997, Precision: 0.1003, Recall: 0.3311, F1-Score: 0.1539\n",
      "Checkpoint saved for epoch 3\n",
      "\n",
      "Epoch 4/10\n",
      "Step 0, Loss: 0.7008339166641235\n",
      "Step 10, Loss: 0.9295236468315125\n",
      "Step 20, Loss: -1.5952807664871216\n",
      "Step 30, Loss: 0.4143986105918884\n",
      "Step 40, Loss: 0.041667789220809937\n",
      "Step 50, Loss: 0.6474341750144958\n",
      "Step 60, Loss: -0.9061659574508667\n",
      "Step 70, Loss: 0.8969202041625977\n",
      "Step 80, Loss: 2.304213285446167\n",
      "Step 90, Loss: 2.491140365600586\n",
      "Step 100, Loss: -0.4408760368824005\n",
      "Step 110, Loss: -1.4043605327606201\n",
      "Step 120, Loss: 1.064090371131897\n",
      "Step 130, Loss: -0.040234118700027466\n",
      "Step 140, Loss: -0.8931757211685181\n",
      "Step 150, Loss: 0.598723292350769\n",
      "Step 160, Loss: 0.5246453881263733\n",
      "Step 170, Loss: 0.09472030401229858\n",
      "Step 180, Loss: 0.24964821338653564\n",
      "Step 190, Loss: -0.37628424167633057\n",
      "Step 200, Loss: -0.6177520155906677\n",
      "Step 210, Loss: -1.8097965717315674\n",
      "Step 220, Loss: -0.2757313847541809\n",
      "Step 230, Loss: 1.372305989265442\n",
      "Step 240, Loss: -0.5575293302536011\n",
      "Step 250, Loss: 0.8362592458724976\n",
      "Step 260, Loss: 0.48812466859817505\n",
      "Step 270, Loss: 0.4943947494029999\n",
      "Step 280, Loss: -0.4858528971672058\n",
      "Step 290, Loss: -1.4771382808685303\n",
      "Step 300, Loss: -1.190507411956787\n",
      "Step 310, Loss: 0.29275238513946533\n",
      "Step 320, Loss: -0.43414306640625\n",
      "Step 330, Loss: -0.11710810661315918\n",
      "Step 340, Loss: 0.16090834140777588\n",
      "Step 350, Loss: -1.6393417119979858\n",
      "Step 360, Loss: 0.3344878554344177\n",
      "Step 370, Loss: 2.162428379058838\n",
      "Step 380, Loss: 1.550783634185791\n",
      "Step 390, Loss: -0.7432820796966553\n",
      "Step 400, Loss: 1.7199606895446777\n",
      "Step 410, Loss: -2.089907169342041\n",
      "Step 420, Loss: 0.08918732404708862\n",
      "Step 430, Loss: 1.298479676246643\n",
      "Step 440, Loss: -0.45598316192626953\n",
      "Step 450, Loss: 0.5933577418327332\n",
      "Step 460, Loss: -1.42426598072052\n",
      "Step 470, Loss: 0.2638695240020752\n",
      "Step 480, Loss: -0.07741150259971619\n",
      "Step 490, Loss: 0.30938956141471863\n",
      "Step 500, Loss: 0.587273359298706\n",
      "Step 510, Loss: 0.17838910222053528\n",
      "Step 520, Loss: 1.8605939149856567\n",
      "Step 530, Loss: -0.6167144179344177\n",
      "Step 540, Loss: 0.5774029493331909\n",
      "Step 550, Loss: -1.089576005935669\n",
      "Step 560, Loss: -1.2721314430236816\n",
      "Step 570, Loss: -0.8223037719726562\n",
      "Step 580, Loss: -1.4459320306777954\n",
      "Step 590, Loss: 1.3565261363983154\n",
      "Step 600, Loss: 0.48467469215393066\n",
      "Step 610, Loss: -0.31411099433898926\n",
      "Step 620, Loss: -1.1971750259399414\n",
      "Step 630, Loss: 0.6975958347320557\n",
      "Step 640, Loss: 1.1483371257781982\n",
      "Step 650, Loss: 0.580855131149292\n",
      "Step 660, Loss: -0.9017906188964844\n",
      "Step 670, Loss: -0.673759937286377\n",
      "Step 680, Loss: -1.3529367446899414\n",
      "Step 690, Loss: 1.2996857166290283\n",
      "Step 700, Loss: -0.26227912306785583\n",
      "Step 710, Loss: -1.304689645767212\n",
      "Step 720, Loss: -0.861379861831665\n",
      "Step 730, Loss: 0.02936643362045288\n",
      "Step 740, Loss: -0.47386831045150757\n",
      "Step 750, Loss: -1.1745424270629883\n",
      "Step 760, Loss: 0.37742578983306885\n",
      "Step 770, Loss: -0.48513275384902954\n",
      "Step 780, Loss: 0.2819923758506775\n",
      "Epoch 4 - Average Loss: 0.0192, Accuracy: 0.0979, Precision: 0.0994, Recall: 0.2921, F1-Score: 0.1484\n",
      "Checkpoint saved for epoch 4\n",
      "\n",
      "Epoch 5/10\n",
      "Step 0, Loss: -0.9314014911651611\n",
      "Step 10, Loss: -0.7390989065170288\n",
      "Step 20, Loss: 0.7226011157035828\n",
      "Step 30, Loss: 0.25898510217666626\n",
      "Step 40, Loss: 0.27643606066703796\n",
      "Step 50, Loss: -1.7867238521575928\n",
      "Step 60, Loss: -0.15153658390045166\n",
      "Step 70, Loss: 1.0249409675598145\n",
      "Step 80, Loss: 0.6584693193435669\n",
      "Step 90, Loss: 1.0247511863708496\n",
      "Step 100, Loss: 0.38326555490493774\n",
      "Step 110, Loss: -0.08550387620925903\n",
      "Step 120, Loss: 0.6234738826751709\n",
      "Step 130, Loss: 1.256610631942749\n",
      "Step 140, Loss: -0.29409265518188477\n",
      "Step 150, Loss: 1.6417744159698486\n",
      "Step 160, Loss: -0.1530584692955017\n",
      "Step 170, Loss: 0.2625845968723297\n",
      "Step 180, Loss: -0.9814216494560242\n",
      "Step 190, Loss: -0.4212731122970581\n",
      "Step 200, Loss: 1.284578561782837\n",
      "Step 210, Loss: -0.43842703104019165\n",
      "Step 220, Loss: 0.6950253248214722\n",
      "Step 230, Loss: -2.0688529014587402\n",
      "Step 240, Loss: -0.623138964176178\n",
      "Step 250, Loss: -1.3448741436004639\n",
      "Step 260, Loss: -0.5453652739524841\n",
      "Step 270, Loss: 0.9204037189483643\n",
      "Step 280, Loss: -0.09190048277378082\n",
      "Step 290, Loss: 0.2509126663208008\n",
      "Step 300, Loss: -0.9737686514854431\n",
      "Step 310, Loss: -1.5081589221954346\n",
      "Step 320, Loss: 1.6887774467468262\n",
      "Step 330, Loss: 2.8903212547302246\n",
      "Step 340, Loss: -3.2907025814056396\n",
      "Step 350, Loss: -0.26140108704566956\n",
      "Step 360, Loss: -0.7967902421951294\n",
      "Step 370, Loss: 0.7109979391098022\n",
      "Step 380, Loss: -3.3382201194763184\n",
      "Step 390, Loss: 1.687987208366394\n",
      "Step 400, Loss: -0.8491723537445068\n",
      "Step 410, Loss: -0.33438634872436523\n",
      "Step 420, Loss: -0.8032667636871338\n",
      "Step 430, Loss: 0.880477786064148\n",
      "Step 440, Loss: 1.1148797273635864\n",
      "Step 450, Loss: 0.26301679015159607\n",
      "Step 460, Loss: 0.4477088451385498\n",
      "Step 470, Loss: 1.3773808479309082\n",
      "Step 480, Loss: -0.32059305906295776\n",
      "Step 490, Loss: -0.3727753758430481\n",
      "Step 500, Loss: 0.033197224140167236\n",
      "Step 510, Loss: 0.37863507866859436\n",
      "Step 520, Loss: -0.49261170625686646\n",
      "Step 530, Loss: -0.7421300411224365\n",
      "Step 540, Loss: -1.3702417612075806\n",
      "Step 550, Loss: -1.2555196285247803\n",
      "Step 560, Loss: -0.9034161567687988\n",
      "Step 570, Loss: 0.3411310017108917\n",
      "Step 580, Loss: -0.3926064670085907\n",
      "Step 590, Loss: -1.8399786949157715\n",
      "Step 600, Loss: -1.4508123397827148\n",
      "Step 610, Loss: -0.8889187574386597\n",
      "Step 620, Loss: -0.4176546633243561\n",
      "Step 630, Loss: 0.4157775342464447\n",
      "Step 640, Loss: 1.6871188879013062\n",
      "Step 650, Loss: 0.9468065500259399\n",
      "Step 660, Loss: 0.23761330544948578\n",
      "Step 670, Loss: 0.9037566184997559\n",
      "Step 680, Loss: -0.40050190687179565\n",
      "Step 690, Loss: 0.48990482091903687\n",
      "Step 700, Loss: 0.1535368114709854\n",
      "Step 710, Loss: 0.14775051176548004\n",
      "Step 720, Loss: -1.551944375038147\n",
      "Step 730, Loss: -0.018948912620544434\n",
      "Step 740, Loss: 0.8578824996948242\n",
      "Step 750, Loss: 1.907440423965454\n",
      "Step 760, Loss: -0.359965980052948\n",
      "Step 770, Loss: -0.18433764576911926\n",
      "Step 780, Loss: -1.7010598182678223\n",
      "Epoch 5 - Average Loss: 0.0644, Accuracy: 0.1016, Precision: 0.1007, Recall: 0.2554, F1-Score: 0.1445\n",
      "Checkpoint saved for epoch 5\n",
      "\n",
      "Epoch 6/10\n",
      "Step 0, Loss: -0.8571087121963501\n",
      "Step 10, Loss: -1.3380324840545654\n",
      "Step 20, Loss: 0.08386549353599548\n",
      "Step 30, Loss: -0.2536385655403137\n",
      "Step 40, Loss: -0.5954106450080872\n",
      "Step 50, Loss: 0.04533928632736206\n",
      "Step 60, Loss: 0.0697445273399353\n",
      "Step 70, Loss: 0.3216872215270996\n",
      "Step 80, Loss: 0.3496968746185303\n",
      "Step 90, Loss: -0.2814897298812866\n",
      "Step 100, Loss: 1.896288514137268\n",
      "Step 110, Loss: 0.9255677461624146\n",
      "Step 120, Loss: 0.4240407943725586\n",
      "Step 130, Loss: -0.35014498233795166\n",
      "Step 140, Loss: -0.24458250403404236\n",
      "Step 150, Loss: -1.0232751369476318\n",
      "Step 160, Loss: -0.17830264568328857\n",
      "Step 170, Loss: -0.08494436740875244\n",
      "Step 180, Loss: 0.1286364644765854\n",
      "Step 190, Loss: 0.05328938364982605\n",
      "Step 200, Loss: 0.5972702503204346\n",
      "Step 210, Loss: 0.23849999904632568\n",
      "Step 220, Loss: -0.04152566194534302\n",
      "Step 230, Loss: 0.7713160514831543\n",
      "Step 240, Loss: -0.21448887884616852\n",
      "Step 250, Loss: -0.8888524770736694\n",
      "Step 260, Loss: 0.5548478364944458\n",
      "Step 270, Loss: -0.09262001514434814\n",
      "Step 280, Loss: -1.3319063186645508\n",
      "Step 290, Loss: 0.1268693208694458\n",
      "Step 300, Loss: 0.8699420094490051\n",
      "Step 310, Loss: -0.12447965145111084\n",
      "Step 320, Loss: -0.8178598880767822\n",
      "Step 330, Loss: 0.16734284162521362\n",
      "Step 340, Loss: -0.17256569862365723\n",
      "Step 350, Loss: -1.0822277069091797\n",
      "Step 360, Loss: 0.0605044960975647\n",
      "Step 370, Loss: -0.10119783878326416\n",
      "Step 380, Loss: -1.0372116565704346\n",
      "Step 390, Loss: -1.3193165063858032\n",
      "Step 400, Loss: 1.1856684684753418\n",
      "Step 410, Loss: 0.31677958369255066\n",
      "Step 420, Loss: 1.548796534538269\n",
      "Step 430, Loss: -1.0150864124298096\n",
      "Step 440, Loss: 1.1988518238067627\n",
      "Step 450, Loss: -1.0236824750900269\n",
      "Step 460, Loss: -1.1898279190063477\n",
      "Step 470, Loss: -0.609250545501709\n",
      "Step 480, Loss: 0.223639577627182\n",
      "Step 490, Loss: 1.151196002960205\n",
      "Step 500, Loss: 0.17237988114356995\n",
      "Step 510, Loss: 0.1935068666934967\n",
      "Step 520, Loss: -1.1972169876098633\n",
      "Step 530, Loss: -1.0494633913040161\n",
      "Step 540, Loss: 0.7192837595939636\n",
      "Step 550, Loss: 0.7017443180084229\n",
      "Step 560, Loss: -1.4189461469650269\n",
      "Step 570, Loss: 0.02948540449142456\n",
      "Step 580, Loss: 0.7197030782699585\n",
      "Step 590, Loss: 1.0896960496902466\n",
      "Step 600, Loss: -1.0324671268463135\n",
      "Step 610, Loss: -0.04182112216949463\n",
      "Step 620, Loss: 1.3403621912002563\n",
      "Step 630, Loss: 0.5334307551383972\n",
      "Step 640, Loss: 0.7288268208503723\n",
      "Step 650, Loss: -1.781736135482788\n",
      "Step 660, Loss: 0.017002522945404053\n",
      "Step 670, Loss: -0.5203126668930054\n",
      "Step 680, Loss: -0.2537508010864258\n",
      "Step 690, Loss: 0.03851999342441559\n",
      "Step 700, Loss: 0.07908016443252563\n",
      "Step 710, Loss: -0.6396573781967163\n",
      "Step 720, Loss: 1.4295862913131714\n",
      "Step 730, Loss: -0.679389476776123\n",
      "Step 740, Loss: 0.9329895377159119\n",
      "Step 750, Loss: -0.318960577249527\n",
      "Step 760, Loss: 0.019289910793304443\n",
      "Step 770, Loss: 1.3409645557403564\n",
      "Step 780, Loss: 1.2441376447677612\n",
      "Epoch 6 - Average Loss: 0.0268, Accuracy: 0.1010, Precision: 0.0998, Recall: 0.2844, F1-Score: 0.1477\n",
      "Checkpoint saved for epoch 6\n",
      "\n",
      "Epoch 7/10\n",
      "Step 0, Loss: 1.0688223838806152\n",
      "Step 10, Loss: -0.16746538877487183\n",
      "Step 20, Loss: 0.5382006168365479\n",
      "Step 30, Loss: 1.1719167232513428\n",
      "Step 40, Loss: -1.026247501373291\n",
      "Step 50, Loss: -0.9877244234085083\n",
      "Step 60, Loss: 0.3866496682167053\n",
      "Step 70, Loss: 1.1443687677383423\n",
      "Step 80, Loss: -1.7489380836486816\n",
      "Step 90, Loss: -1.2886533737182617\n",
      "Step 100, Loss: 0.02302706241607666\n",
      "Step 110, Loss: -0.11370277404785156\n",
      "Step 120, Loss: 0.5539716482162476\n",
      "Step 130, Loss: -0.5298391580581665\n",
      "Step 140, Loss: 0.0029603242874145508\n",
      "Step 150, Loss: -1.213448166847229\n",
      "Step 160, Loss: 0.9597035646438599\n",
      "Step 170, Loss: 1.5353058576583862\n",
      "Step 180, Loss: -0.6909536123275757\n",
      "Step 190, Loss: -0.7536038756370544\n",
      "Step 200, Loss: -2.6862034797668457\n",
      "Step 210, Loss: 1.119711995124817\n",
      "Step 220, Loss: -0.31618985533714294\n",
      "Step 230, Loss: 0.1836947202682495\n",
      "Step 240, Loss: -1.9346319437026978\n",
      "Step 250, Loss: 0.10456213355064392\n",
      "Step 260, Loss: 0.3228873312473297\n",
      "Step 270, Loss: -2.3019158840179443\n",
      "Step 280, Loss: -0.06881234049797058\n",
      "Step 290, Loss: 0.4030348062515259\n",
      "Step 300, Loss: -0.3846777081489563\n",
      "Step 310, Loss: 0.6731592416763306\n",
      "Step 320, Loss: 0.5313267707824707\n",
      "Step 330, Loss: -1.0542833805084229\n",
      "Step 340, Loss: 0.230872243642807\n",
      "Step 350, Loss: 0.3520510196685791\n",
      "Step 360, Loss: 0.20130139589309692\n",
      "Step 370, Loss: -0.5729709267616272\n",
      "Step 380, Loss: -1.5542200803756714\n",
      "Step 390, Loss: -0.050304114818573\n",
      "Step 400, Loss: 0.8560704588890076\n",
      "Step 410, Loss: 1.6124082803726196\n",
      "Step 420, Loss: 0.2698225975036621\n",
      "Step 430, Loss: -0.6547354459762573\n",
      "Step 440, Loss: 1.6747759580612183\n",
      "Step 450, Loss: -0.02552257478237152\n",
      "Step 460, Loss: -0.09653663635253906\n",
      "Step 470, Loss: 0.2768723666667938\n",
      "Step 480, Loss: -1.229604721069336\n",
      "Step 490, Loss: -0.36589422821998596\n",
      "Step 500, Loss: 1.3095349073410034\n",
      "Step 510, Loss: 1.7698591947555542\n",
      "Step 520, Loss: 0.6224011182785034\n",
      "Step 530, Loss: -0.7228667736053467\n",
      "Step 540, Loss: 1.6172661781311035\n",
      "Step 550, Loss: -1.0585918426513672\n",
      "Step 560, Loss: 0.17980650067329407\n",
      "Step 570, Loss: 0.2524161636829376\n",
      "Step 580, Loss: 0.2877020537853241\n",
      "Step 590, Loss: 0.4999430775642395\n",
      "Step 600, Loss: 1.0461900234222412\n",
      "Step 610, Loss: 0.9210019111633301\n",
      "Step 620, Loss: 0.9196535348892212\n",
      "Step 630, Loss: 0.8648011088371277\n",
      "Step 640, Loss: 1.1850981712341309\n",
      "Step 650, Loss: -0.05180013179779053\n",
      "Step 660, Loss: 0.46463921666145325\n",
      "Step 670, Loss: -1.2654427289962769\n",
      "Step 680, Loss: 0.05537259578704834\n",
      "Step 690, Loss: -0.5725798606872559\n",
      "Step 700, Loss: 0.461070716381073\n",
      "Step 710, Loss: -0.8822604417800903\n",
      "Step 720, Loss: 1.2068231105804443\n",
      "Step 730, Loss: 1.3077505826950073\n",
      "Step 740, Loss: 0.7048627734184265\n",
      "Step 750, Loss: 0.2706280052661896\n",
      "Step 760, Loss: -1.7344306707382202\n",
      "Step 770, Loss: -0.021050631999969482\n",
      "Step 780, Loss: 1.1883313655853271\n",
      "Epoch 7 - Average Loss: -0.0043, Accuracy: 0.1002, Precision: 0.1000, Recall: 0.4447, F1-Score: 0.1633\n",
      "Checkpoint saved for epoch 7\n",
      "\n",
      "Epoch 8/10\n",
      "Step 0, Loss: -0.30491772294044495\n",
      "Step 10, Loss: -0.7480483055114746\n",
      "Step 20, Loss: 0.2223157286643982\n",
      "Step 30, Loss: -0.5735282897949219\n",
      "Step 40, Loss: 0.4855308532714844\n",
      "Step 50, Loss: -0.5869810581207275\n",
      "Step 60, Loss: -0.9648159742355347\n",
      "Step 70, Loss: 0.350824773311615\n",
      "Step 80, Loss: 0.237909197807312\n",
      "Step 90, Loss: 0.11286163330078125\n",
      "Step 100, Loss: 0.7779258489608765\n",
      "Step 110, Loss: 1.0399266481399536\n",
      "Step 120, Loss: -0.09699547290802002\n",
      "Step 130, Loss: -0.3358003497123718\n",
      "Step 140, Loss: 0.3810693919658661\n",
      "Step 150, Loss: 0.765937328338623\n",
      "Step 160, Loss: 0.812840461730957\n",
      "Step 170, Loss: -0.7929925322532654\n",
      "Step 180, Loss: -1.2178294658660889\n",
      "Step 190, Loss: -1.1965992450714111\n",
      "Step 200, Loss: -1.143195629119873\n",
      "Step 210, Loss: -0.9730914831161499\n",
      "Step 220, Loss: -0.46782976388931274\n",
      "Step 230, Loss: 1.1794849634170532\n",
      "Step 240, Loss: 0.7552456855773926\n",
      "Step 250, Loss: 0.3913877010345459\n",
      "Step 260, Loss: 2.6561903953552246\n",
      "Step 270, Loss: 2.1972389221191406\n",
      "Step 280, Loss: 1.5799609422683716\n",
      "Step 290, Loss: -0.763858437538147\n",
      "Step 300, Loss: -0.4373338222503662\n",
      "Step 310, Loss: 0.16309231519699097\n",
      "Step 320, Loss: -0.7435811161994934\n",
      "Step 330, Loss: -1.311844825744629\n",
      "Step 340, Loss: 0.16960763931274414\n",
      "Step 350, Loss: 0.1391238272190094\n",
      "Step 360, Loss: 0.1314275860786438\n",
      "Step 370, Loss: -1.6866540908813477\n",
      "Step 380, Loss: -0.04137274622917175\n",
      "Step 390, Loss: 1.7707948684692383\n",
      "Step 400, Loss: -0.9490098357200623\n",
      "Step 410, Loss: 0.6618885397911072\n",
      "Step 420, Loss: -0.7367979288101196\n",
      "Step 430, Loss: -0.7821235060691833\n",
      "Step 440, Loss: 0.6628990173339844\n",
      "Step 450, Loss: 0.5303627848625183\n",
      "Step 460, Loss: 0.5221794843673706\n",
      "Step 470, Loss: 0.5563490986824036\n",
      "Step 480, Loss: -0.26748716831207275\n",
      "Step 490, Loss: 0.9494647979736328\n",
      "Step 500, Loss: 0.8622686266899109\n",
      "Step 510, Loss: 2.0559701919555664\n",
      "Step 520, Loss: -1.4835047721862793\n",
      "Step 530, Loss: -0.09463021159172058\n",
      "Step 540, Loss: -1.0504493713378906\n",
      "Step 550, Loss: -1.0356671810150146\n",
      "Step 560, Loss: -1.1434381008148193\n",
      "Step 570, Loss: 0.565254271030426\n",
      "Step 580, Loss: 0.4404970407485962\n",
      "Step 590, Loss: -0.8461394309997559\n",
      "Step 600, Loss: 1.2921340465545654\n",
      "Step 610, Loss: -1.1496247053146362\n",
      "Step 620, Loss: 0.8325521349906921\n",
      "Step 630, Loss: -0.2917061448097229\n",
      "Step 640, Loss: -1.5590256452560425\n",
      "Step 650, Loss: 1.0183300971984863\n",
      "Step 660, Loss: -0.3465079367160797\n",
      "Step 670, Loss: 0.9930092096328735\n",
      "Step 680, Loss: -0.010707437992095947\n",
      "Step 690, Loss: -0.3952590227127075\n",
      "Step 700, Loss: 1.0230331420898438\n",
      "Step 710, Loss: 1.0205004215240479\n",
      "Step 720, Loss: 1.0392879247665405\n",
      "Step 730, Loss: 2.1931724548339844\n",
      "Step 740, Loss: 0.6446236371994019\n",
      "Step 750, Loss: -1.4431207180023193\n",
      "Step 760, Loss: -1.8492624759674072\n",
      "Step 770, Loss: 2.0947890281677246\n",
      "Step 780, Loss: 1.316131591796875\n",
      "Epoch 8 - Average Loss: 0.0038, Accuracy: 0.1004, Precision: 0.0998, Recall: 0.4515, F1-Score: 0.1635\n",
      "Checkpoint saved for epoch 8\n",
      "\n",
      "Epoch 9/10\n",
      "Step 0, Loss: -0.3984016180038452\n",
      "Step 10, Loss: -1.3728752136230469\n",
      "Step 20, Loss: -0.1259462833404541\n",
      "Step 30, Loss: 0.981818675994873\n",
      "Step 40, Loss: 1.1967155933380127\n",
      "Step 50, Loss: -0.5395445823669434\n",
      "Step 60, Loss: -0.028723731637001038\n",
      "Step 70, Loss: -0.33304768800735474\n",
      "Step 80, Loss: 0.3572208881378174\n",
      "Step 90, Loss: 0.21313822269439697\n",
      "Step 100, Loss: 0.048048585653305054\n",
      "Step 110, Loss: 0.14108574390411377\n",
      "Step 120, Loss: 0.7997143268585205\n",
      "Step 130, Loss: 1.501382827758789\n",
      "Step 140, Loss: -0.008211076259613037\n",
      "Step 150, Loss: 0.09655141830444336\n",
      "Step 160, Loss: -0.39969319105148315\n",
      "Step 170, Loss: -0.8339400291442871\n",
      "Step 180, Loss: 2.5994157791137695\n",
      "Step 190, Loss: 0.18302083015441895\n",
      "Step 200, Loss: -0.6067761778831482\n",
      "Step 210, Loss: -0.28441137075424194\n",
      "Step 220, Loss: -0.28381383419036865\n",
      "Step 230, Loss: -0.9526466131210327\n",
      "Step 240, Loss: -1.7763463258743286\n",
      "Step 250, Loss: 0.20516318082809448\n",
      "Step 260, Loss: -0.7073947191238403\n",
      "Step 270, Loss: -0.0911170244216919\n",
      "Step 280, Loss: 1.4024834632873535\n",
      "Step 290, Loss: 0.41603875160217285\n",
      "Step 300, Loss: -1.0030490159988403\n",
      "Step 310, Loss: -1.2823783159255981\n",
      "Step 320, Loss: -0.1792711317539215\n",
      "Step 330, Loss: 0.34169909358024597\n",
      "Step 340, Loss: 0.969822883605957\n",
      "Step 350, Loss: 1.4195940494537354\n",
      "Step 360, Loss: 1.3221882581710815\n",
      "Step 370, Loss: -1.1061056852340698\n",
      "Step 380, Loss: -0.7360103130340576\n",
      "Step 390, Loss: -0.057996928691864014\n",
      "Step 400, Loss: 0.624711811542511\n",
      "Step 410, Loss: 0.43332526087760925\n",
      "Step 420, Loss: -0.35766226053237915\n",
      "Step 430, Loss: 0.5273253917694092\n",
      "Step 440, Loss: 0.08190184831619263\n",
      "Step 450, Loss: 1.4196244478225708\n",
      "Step 460, Loss: 1.7355705499649048\n",
      "Step 470, Loss: 0.5366253852844238\n",
      "Step 480, Loss: 0.15885192155838013\n",
      "Step 490, Loss: -3.131589412689209\n",
      "Step 500, Loss: -0.9313430786132812\n",
      "Step 510, Loss: -0.6987038850784302\n",
      "Step 520, Loss: 1.0528162717819214\n",
      "Step 530, Loss: 0.6828839778900146\n",
      "Step 540, Loss: 0.6229316592216492\n",
      "Step 550, Loss: 0.2205827832221985\n",
      "Step 560, Loss: -0.23093369603157043\n",
      "Step 570, Loss: 0.14709782600402832\n",
      "Step 580, Loss: -0.525695264339447\n",
      "Step 590, Loss: -0.5646663904190063\n",
      "Step 600, Loss: 0.6457912921905518\n",
      "Step 610, Loss: 1.7829675674438477\n",
      "Step 620, Loss: 0.28328511118888855\n",
      "Step 630, Loss: -0.2726758122444153\n",
      "Step 640, Loss: 0.1865931749343872\n",
      "Step 650, Loss: -0.8251221179962158\n",
      "Step 660, Loss: -0.645429253578186\n",
      "Step 670, Loss: -1.022036075592041\n",
      "Step 680, Loss: 0.3174659013748169\n",
      "Step 690, Loss: -1.29225492477417\n",
      "Step 700, Loss: 0.9647276401519775\n",
      "Step 710, Loss: 0.6050525903701782\n",
      "Step 720, Loss: -1.7230242490768433\n",
      "Step 730, Loss: -2.054673194885254\n",
      "Step 740, Loss: -1.4417452812194824\n",
      "Step 750, Loss: 0.25604820251464844\n",
      "Step 760, Loss: 1.3573126792907715\n",
      "Step 770, Loss: -0.16203737258911133\n",
      "Step 780, Loss: -0.778746485710144\n",
      "Epoch 9 - Average Loss: 0.0037, Accuracy: 0.1000, Precision: 0.1000, Recall: 0.4153, F1-Score: 0.1612\n",
      "Checkpoint saved for epoch 9\n",
      "\n",
      "Epoch 10/10\n",
      "Step 0, Loss: -0.33287832140922546\n",
      "Step 10, Loss: 0.5978227853775024\n",
      "Step 20, Loss: -0.4583439230918884\n",
      "Step 30, Loss: 0.3452388048171997\n",
      "Step 40, Loss: 1.091563105583191\n",
      "Step 50, Loss: -1.593151569366455\n",
      "Step 60, Loss: 1.085525631904602\n",
      "Step 70, Loss: 0.3655264377593994\n",
      "Step 80, Loss: 0.163534015417099\n",
      "Step 90, Loss: 0.4521915912628174\n",
      "Step 100, Loss: 0.45861637592315674\n",
      "Step 110, Loss: -1.3815362453460693\n",
      "Step 120, Loss: -0.9221180081367493\n",
      "Step 130, Loss: 0.5831515789031982\n",
      "Step 140, Loss: 0.045848339796066284\n",
      "Step 150, Loss: 1.2543187141418457\n",
      "Step 160, Loss: -0.48526281118392944\n",
      "Step 170, Loss: 0.40523216128349304\n",
      "Step 180, Loss: -0.5553561449050903\n",
      "Step 190, Loss: 0.33778810501098633\n",
      "Step 200, Loss: 1.045951247215271\n",
      "Step 210, Loss: 0.27435243129730225\n",
      "Step 220, Loss: 0.7621008157730103\n",
      "Step 230, Loss: 0.061269611120224\n",
      "Step 240, Loss: -0.61886066198349\n",
      "Step 250, Loss: -0.7713695764541626\n",
      "Step 260, Loss: 0.4217800498008728\n",
      "Step 270, Loss: -0.8920881748199463\n",
      "Step 280, Loss: 0.026427894830703735\n",
      "Step 290, Loss: -0.24102327227592468\n",
      "Step 300, Loss: 1.637519359588623\n",
      "Step 310, Loss: 0.3003820478916168\n",
      "Step 320, Loss: 0.13286887109279633\n",
      "Step 330, Loss: 0.7711942195892334\n",
      "Step 340, Loss: -0.5461118221282959\n",
      "Step 350, Loss: -0.6908189058303833\n",
      "Step 360, Loss: -0.18498724699020386\n",
      "Step 370, Loss: 1.2509446144104004\n",
      "Step 380, Loss: -1.1070441007614136\n",
      "Step 390, Loss: 1.329183578491211e-05\n",
      "Step 400, Loss: -0.3340197801589966\n",
      "Step 410, Loss: 1.4521753787994385\n",
      "Step 420, Loss: 2.123704195022583\n",
      "Step 430, Loss: 0.5299860835075378\n",
      "Step 440, Loss: 1.1812459230422974\n",
      "Step 450, Loss: -0.043640196323394775\n",
      "Step 460, Loss: 0.5679070949554443\n",
      "Step 470, Loss: 0.5737165212631226\n",
      "Step 480, Loss: -0.45669734477996826\n",
      "Step 490, Loss: 1.6892352104187012\n",
      "Step 500, Loss: -0.37365472316741943\n",
      "Step 510, Loss: 0.3583023250102997\n",
      "Step 520, Loss: -0.4531922936439514\n",
      "Step 530, Loss: 0.5575059652328491\n",
      "Step 540, Loss: -0.4238927960395813\n",
      "Step 550, Loss: 0.9552294611930847\n",
      "Step 560, Loss: 0.4221246838569641\n",
      "Step 570, Loss: -1.1292753219604492\n",
      "Step 580, Loss: -0.698208212852478\n",
      "Step 590, Loss: -0.06354767829179764\n",
      "Step 600, Loss: 0.06242818385362625\n",
      "Step 610, Loss: -0.5420153737068176\n",
      "Step 620, Loss: 1.5176043510437012\n",
      "Step 630, Loss: 0.48063212633132935\n",
      "Step 640, Loss: 0.7849090099334717\n",
      "Step 650, Loss: -0.9577823281288147\n",
      "Step 660, Loss: -0.12145400047302246\n",
      "Step 670, Loss: -0.9600353837013245\n",
      "Step 680, Loss: -0.5181390047073364\n",
      "Step 690, Loss: 1.0141576528549194\n",
      "Step 700, Loss: 1.0278730392456055\n",
      "Step 710, Loss: -0.32977208495140076\n",
      "Step 720, Loss: 0.21561774611473083\n",
      "Step 730, Loss: 0.3861428499221802\n",
      "Step 740, Loss: -0.1709367036819458\n",
      "Step 750, Loss: 3.5951924324035645\n",
      "Step 760, Loss: 1.9789847135543823\n",
      "Step 770, Loss: -0.24436123669147491\n",
      "Step 780, Loss: -2.0519049167633057\n",
      "Epoch 10 - Average Loss: 0.0390, Accuracy: 0.0987, Precision: 0.1000, Recall: 0.3719, F1-Score: 0.1576\n",
      "Checkpoint saved for epoch 10\n",
      "Metrics saved to 'training_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths to label files\n",
    "    cifar10_label_file = \"./data/CIFAR-10_human_ordered.npy\"\n",
    "    cifar100_label_file = \"./data/CIFAR-100_human_ordered.npy\"\n",
    "\n",
    "    # Load CIFAR-10 dataset for training\n",
    "    train_ds_cifar10 = load_combined_dataset(\"cifar10\", cifar10_label_file, num_classes=10)\n",
    "\n",
    "    # Train on CIFAR-10 (CIFAR-100 can be loaded similarly)\n",
    "    train_model(train_ds_cifar10, epochs=10, num_classes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
