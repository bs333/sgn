{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-ratio transformations\n",
    "def clr_inv(p):\n",
    "    z = tf.math.log(p)\n",
    "    return z - tf.reduce_mean(z, axis=1)[:, tf.newaxis]\n",
    "\n",
    "def clr_forward(z, axis=1):\n",
    "    return tf.nn.softmax(z, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_resnet(num_classes, depth=28, width=2):\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "    for _ in range(depth // 6):\n",
    "        x = layers.Conv2D(16 * width, (3, 3), padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)  # Use Keras's ReLU layer\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader combining TF CIFAR datasets with custom labels\n",
    "def load_combined_dataset(cifar_version, label_file, num_classes):\n",
    "    \"\"\"\n",
    "    Combine CIFAR image data from TensorFlow with labels from a .npy file.\n",
    "\n",
    "    Args:\n",
    "        cifar_version (str): \"cifar10\" or \"cifar100\" to select the dataset.\n",
    "        label_file (str): Path to the .npy file containing labels.\n",
    "        num_classes (int): Number of classes (10 or 100).\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset ready for training/testing.\n",
    "    \"\"\"\n",
    "    # Load image data\n",
    "    if cifar_version == \"cifar10\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar10.load_data()\n",
    "    elif cifar_version == \"cifar100\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar100.load_data()\n",
    "    else:\n",
    "        raise ValueError(\"cifar_version must be 'cifar10' or 'cifar100'.\")\n",
    "\n",
    "    # Load labels from .npy file\n",
    "    label_data = np.load(label_file, allow_pickle=True).item()\n",
    "    if 'clean_label' not in label_data:\n",
    "        raise ValueError(\"Label file must contain 'clean_label' key.\")\n",
    "\n",
    "    # Normalize image data and convert labels\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    y_train = tf.keras.utils.to_categorical(label_data['clean_label'], num_classes)\n",
    "\n",
    "    # Create TensorFlow dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian noise (manual implementation)\n",
    "def add_gaussian_noise(inputs, mean=0.0, stddev=0.1):\n",
    "    noise = tf.random.normal(shape=tf.shape(inputs), mean=mean, stddev=stddev)\n",
    "    return inputs + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(model, images, labels, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        smoothed_targets = clr_forward(labels)\n",
    "        logit_targets = clr_inv(smoothed_targets)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(logit_targets, logits))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(train_ds, epochs=10, num_classes=10):\n",
    "    model = wide_resnet(num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for step, (images, labels) in enumerate(train_ds):\n",
    "            loss = train_step(model, images, labels, optimizer)\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        # Save checkpoint with proper extension\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        model.save_weights(f\"checkpoints/sgn_epoch_{epoch}.weights.h5\")\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Step 0, Loss: -0.20432591438293457\n",
      "Step 10, Loss: -0.31404414772987366\n",
      "Step 20, Loss: 0.9734049439430237\n",
      "Step 30, Loss: -1.006782054901123\n",
      "Step 40, Loss: 0.030450940132141113\n",
      "Step 50, Loss: -0.9610440731048584\n",
      "Step 60, Loss: -0.6163628101348877\n",
      "Step 70, Loss: -0.29540014266967773\n",
      "Step 80, Loss: 0.5773115158081055\n",
      "Step 90, Loss: -0.29664847254753113\n",
      "Step 100, Loss: 2.307150363922119\n",
      "Step 110, Loss: -1.5923638343811035\n",
      "Step 120, Loss: 0.5087367296218872\n",
      "Step 130, Loss: 1.4371232986450195\n",
      "Step 140, Loss: -0.7426465749740601\n",
      "Step 150, Loss: 0.14923328161239624\n",
      "Step 160, Loss: 0.5736962556838989\n",
      "Step 170, Loss: 0.3742971420288086\n",
      "Step 180, Loss: 0.2594591975212097\n",
      "Step 190, Loss: -0.05039896070957184\n",
      "Step 200, Loss: 1.5598995685577393\n",
      "Step 210, Loss: -0.3189013600349426\n",
      "Step 220, Loss: 0.5471197366714478\n",
      "Step 230, Loss: -0.6155037879943848\n",
      "Step 240, Loss: 0.2993968725204468\n",
      "Step 250, Loss: -0.6815849542617798\n",
      "Step 260, Loss: 0.94915372133255\n",
      "Step 270, Loss: 0.0054465532302856445\n",
      "Step 280, Loss: 1.5406875610351562\n",
      "Step 290, Loss: -1.226133942604065\n",
      "Step 300, Loss: -0.7152426242828369\n",
      "Step 310, Loss: 0.21477490663528442\n",
      "Step 320, Loss: -0.17486363649368286\n",
      "Step 330, Loss: -0.810524582862854\n",
      "Step 340, Loss: -0.3226641118526459\n",
      "Step 350, Loss: 0.36934036016464233\n",
      "Step 360, Loss: 0.8070950508117676\n",
      "Step 370, Loss: -1.442584753036499\n",
      "Step 380, Loss: -1.6609861850738525\n",
      "Step 390, Loss: 0.11627638339996338\n",
      "Step 400, Loss: -0.916449785232544\n",
      "Step 410, Loss: -0.7736594676971436\n",
      "Step 420, Loss: 0.31493037939071655\n",
      "Step 430, Loss: -0.8819655179977417\n",
      "Step 440, Loss: -0.41985613107681274\n",
      "Step 450, Loss: -2.0501832962036133\n",
      "Step 460, Loss: -0.9317193031311035\n",
      "Step 470, Loss: -0.9613826274871826\n",
      "Step 480, Loss: 0.5189769864082336\n",
      "Step 490, Loss: -0.507482647895813\n",
      "Step 500, Loss: -0.9034425616264343\n",
      "Step 510, Loss: 0.2214345484972\n",
      "Step 520, Loss: -1.991485834121704\n",
      "Step 530, Loss: 0.11073613166809082\n",
      "Step 540, Loss: -1.651348352432251\n",
      "Step 550, Loss: 0.48206716775894165\n",
      "Step 560, Loss: -1.6035912036895752\n",
      "Step 570, Loss: -0.22399744391441345\n",
      "Step 580, Loss: -2.2046868801116943\n",
      "Step 590, Loss: 1.2706096172332764\n",
      "Step 600, Loss: 0.27800625562667847\n",
      "Step 610, Loss: 0.21259862184524536\n",
      "Step 620, Loss: -0.29428714513778687\n",
      "Step 630, Loss: -0.4629385471343994\n",
      "Step 640, Loss: -1.253010630607605\n",
      "Step 650, Loss: -0.722869336605072\n",
      "Step 660, Loss: 0.618416965007782\n",
      "Step 670, Loss: 0.6523119211196899\n",
      "Step 680, Loss: 0.16425496339797974\n",
      "Step 690, Loss: 0.3488299250602722\n",
      "Step 700, Loss: 0.17766177654266357\n",
      "Step 710, Loss: -1.1025248765945435\n",
      "Step 720, Loss: 1.7124682664871216\n",
      "Step 730, Loss: 0.03359308838844299\n",
      "Step 740, Loss: -0.17878049612045288\n",
      "Step 750, Loss: 0.46370792388916016\n",
      "Step 760, Loss: 0.26698678731918335\n",
      "Step 770, Loss: 0.8601510524749756\n",
      "Step 780, Loss: -0.9474577307701111\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filename must end in `.weights.h5`. Received: filepath=checkpoints/sgn_epoch_0.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train_ds_cifar10 \u001b[38;5;241m=\u001b[39m load_combined_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m\"\u001b[39m, cifar10_label_file, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train on CIFAR-10 (CIFAR-100 can be loaded similarly)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds_cifar10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_ds, epochs, num_classes)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n\u001b[0;32m     28\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/sgn_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint saved for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:224\u001b[0m, in \u001b[0;36msave_weights\u001b[1;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.saving.save_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_weights\u001b[39m(model, filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 224\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filename must end in `.weights.h5`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m         )\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         exists \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filepath)\n",
      "\u001b[1;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=checkpoints/sgn_epoch_0.ckpt"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths to label files\n",
    "    cifar10_label_file = \"./data/CIFAR-10_human_ordered.npy\"\n",
    "    cifar100_label_file = \"./data/CIFAR-100_human_ordered.npy\"\n",
    "\n",
    "    # Load CIFAR-10 dataset for training\n",
    "    train_ds_cifar10 = load_combined_dataset(\"cifar10\", cifar10_label_file, num_classes=10)\n",
    "\n",
    "    # Train on CIFAR-10 (CIFAR-100 can be loaded similarly)\n",
    "    train_model(train_ds_cifar10, epochs=10, num_classes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
