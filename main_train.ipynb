{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-ratio transformations\n",
    "def clr_inv(p):\n",
    "    z = tf.math.log(p)\n",
    "    return z - tf.reduce_mean(z, axis=1)[:, tf.newaxis]\n",
    "\n",
    "def clr_forward(z, axis=1):\n",
    "    return tf.nn.softmax(z, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_resnet(num_classes, depth=28, width=2):\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "    for _ in range(depth // 6):\n",
    "        x = layers.Conv2D(16 * width, (3, 3), padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)  # Use Keras's ReLU layer\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes)(x)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader combining TF CIFAR datasets with custom labels\n",
    "def load_combined_dataset(cifar_version, label_file, num_classes):\n",
    "    \"\"\"\n",
    "    Combine CIFAR image data from TensorFlow with labels from a .npy file.\n",
    "\n",
    "    Args:\n",
    "        cifar_version (str): \"cifar10\" or \"cifar100\" to select the dataset.\n",
    "        label_file (str): Path to the .npy file containing labels.\n",
    "        num_classes (int): Number of classes (10 or 100).\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Dataset ready for training/testing.\n",
    "    \"\"\"\n",
    "    # Load image data\n",
    "    if cifar_version == \"cifar10\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar10.load_data()\n",
    "    elif cifar_version == \"cifar100\":\n",
    "        (x_train, _), _ = tf.keras.datasets.cifar100.load_data()\n",
    "    else:\n",
    "        raise ValueError(\"cifar_version must be 'cifar10' or 'cifar100'.\")\n",
    "\n",
    "    # Load labels from .npy file\n",
    "    label_data = np.load(label_file, allow_pickle=True).item()\n",
    "    if 'clean_label' not in label_data:\n",
    "        raise ValueError(\"Label file must contain 'clean_label' key.\")\n",
    "\n",
    "    # Normalize image data and convert labels\n",
    "    x_train = x_train.astype(np.float32) / 255.0\n",
    "    y_train = tf.keras.utils.to_categorical(label_data['clean_label'], num_classes)\n",
    "\n",
    "    # Create TensorFlow dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.shuffle(10000).batch(64)\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian noise (manual implementation)\n",
    "def add_gaussian_noise(inputs, mean=0.0, stddev=0.1):\n",
    "    noise = tf.random.normal(shape=tf.shape(inputs), mean=mean, stddev=stddev)\n",
    "    return inputs + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(model, images, labels, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        smoothed_targets = clr_forward(labels)\n",
    "        logit_targets = clr_inv(smoothed_targets)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(logit_targets, logits))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train_model(train_ds, epochs=10, num_classes=10):\n",
    "    model = wide_resnet(num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # Define metrics\n",
    "    accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    precision_metric = tf.keras.metrics.Precision()\n",
    "    recall_metric = tf.keras.metrics.Recall()\n",
    "\n",
    "    # DataFrame to store metrics\n",
    "    metrics_df = pd.DataFrame(columns=[\"Epoch\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        epoch_loss_sum = 0\n",
    "        num_steps = 0\n",
    "\n",
    "        # Reset metrics at the start of each epoch\n",
    "        accuracy_metric.reset_state()\n",
    "        precision_metric.reset_state()\n",
    "        recall_metric.reset_state()\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_ds):\n",
    "            # Perform training step\n",
    "            loss = train_step(model, images, labels, optimizer)\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss_sum += loss.numpy()\n",
    "            num_steps += 1\n",
    "            predictions = model(images, training=False)\n",
    "            accuracy_metric.update_state(labels, predictions)\n",
    "            precision_metric.update_state(labels, predictions)\n",
    "            recall_metric.update_state(labels, predictions)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        # Compute metrics for the epoch\n",
    "        avg_loss = epoch_loss_sum / num_steps\n",
    "        epoch_accuracy = accuracy_metric.result().numpy()\n",
    "        epoch_precision = precision_metric.result().numpy()\n",
    "        epoch_recall = recall_metric.result().numpy()\n",
    "        epoch_f1_score = 2 * (epoch_precision * epoch_recall) / (epoch_precision + epoch_recall + 1e-7)\n",
    "\n",
    "        # Display metrics\n",
    "        print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}, \"\n",
    "              f\"Accuracy: {epoch_accuracy:.4f}, \"\n",
    "              f\"Precision: {epoch_precision:.4f}, \"\n",
    "              f\"Recall: {epoch_recall:.4f}, \"\n",
    "              f\"F1-Score: {epoch_f1_score:.4f}\")\n",
    "\n",
    "        # Save metrics to DataFrame\n",
    "        metrics_df = pd.concat([\n",
    "            metrics_df,\n",
    "            pd.DataFrame([{\n",
    "                \"Epoch\": epoch + 1,\n",
    "                \"Loss\": avg_loss,\n",
    "                \"Accuracy\": epoch_accuracy,\n",
    "                \"Precision\": epoch_precision,\n",
    "                \"Recall\": epoch_recall,\n",
    "                \"F1-Score\": epoch_f1_score\n",
    "            }])\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        # Save checkpoint with proper extension\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        model.save_weights(f\"checkpoints/sgn_epoch_{epoch}.weights.h5\")\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}\")\n",
    "\n",
    "    # Save metrics to a CSV file\n",
    "    metrics_df.to_csv(\"training_metrics.csv\", index=False)\n",
    "    print(\"Metrics saved to 'training_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Step 0, Loss: -0.645241916179657\n",
      "Step 10, Loss: 0.6563560366630554\n",
      "Step 20, Loss: -0.5557003617286682\n",
      "Step 30, Loss: -0.27176040410995483\n",
      "Step 40, Loss: -1.2542269229888916\n",
      "Step 50, Loss: 0.6806355714797974\n",
      "Step 60, Loss: 0.7235382199287415\n",
      "Step 70, Loss: -1.1167190074920654\n",
      "Step 80, Loss: -1.1450221538543701\n",
      "Step 90, Loss: 0.25385603308677673\n",
      "Step 100, Loss: 0.24620550870895386\n",
      "Step 110, Loss: -1.9748549461364746\n",
      "Step 120, Loss: 0.96517014503479\n",
      "Step 130, Loss: 1.1253488063812256\n",
      "Step 140, Loss: -1.6284011602401733\n",
      "Step 150, Loss: -0.13061830401420593\n",
      "Step 160, Loss: 2.3540291786193848\n",
      "Step 170, Loss: 0.31986290216445923\n",
      "Step 180, Loss: 0.9701112508773804\n",
      "Step 190, Loss: 0.0657343864440918\n",
      "Step 200, Loss: 1.8304866552352905\n",
      "Step 210, Loss: -0.7666921615600586\n",
      "Step 220, Loss: -0.025174498558044434\n",
      "Step 230, Loss: 1.3266627788543701\n",
      "Step 240, Loss: -0.547177255153656\n",
      "Step 250, Loss: -1.348539113998413\n",
      "Step 260, Loss: -0.17036569118499756\n",
      "Step 270, Loss: -0.2681874632835388\n",
      "Step 280, Loss: 0.5704010128974915\n",
      "Step 290, Loss: -1.8303730487823486\n",
      "Step 300, Loss: -1.7074694633483887\n",
      "Step 310, Loss: 0.13962936401367188\n",
      "Step 320, Loss: -1.604259729385376\n",
      "Step 330, Loss: 1.0418769121170044\n",
      "Step 340, Loss: 0.15847840905189514\n",
      "Step 350, Loss: 0.7726709842681885\n",
      "Step 360, Loss: -0.6338493824005127\n",
      "Step 370, Loss: -0.2500453591346741\n",
      "Step 380, Loss: -1.392280101776123\n",
      "Step 390, Loss: 0.07609765231609344\n",
      "Step 400, Loss: 3.368922233581543\n",
      "Step 410, Loss: -1.136465072631836\n",
      "Step 420, Loss: 1.9586207866668701\n",
      "Step 430, Loss: 0.5536776781082153\n",
      "Step 440, Loss: 1.7748754024505615\n",
      "Step 450, Loss: 1.3663700819015503\n",
      "Step 460, Loss: -0.5648106932640076\n",
      "Step 470, Loss: 0.7271710634231567\n",
      "Step 480, Loss: 0.8260893225669861\n",
      "Step 490, Loss: 0.6106423735618591\n",
      "Step 500, Loss: 0.4472951889038086\n",
      "Step 510, Loss: -0.2916026711463928\n",
      "Step 520, Loss: 0.3271951973438263\n",
      "Step 530, Loss: -1.6966078281402588\n",
      "Step 540, Loss: -0.23663246631622314\n",
      "Step 550, Loss: 0.9503016471862793\n",
      "Step 560, Loss: 0.9499310851097107\n",
      "Step 570, Loss: -0.718738317489624\n",
      "Step 580, Loss: 0.3498830497264862\n",
      "Step 590, Loss: -0.5663084983825684\n",
      "Step 600, Loss: -0.07976295053958893\n",
      "Step 610, Loss: -0.33179008960723877\n",
      "Step 620, Loss: 1.2674250602722168\n",
      "Step 630, Loss: -0.3554229736328125\n",
      "Step 640, Loss: 0.42857635021209717\n",
      "Step 650, Loss: 1.042698621749878\n",
      "Step 660, Loss: 0.26800891757011414\n",
      "Step 670, Loss: 0.8809689879417419\n",
      "Step 680, Loss: -0.6382083892822266\n",
      "Step 690, Loss: -0.19663545489311218\n",
      "Step 700, Loss: -0.8939555883407593\n",
      "Step 710, Loss: -0.4219646751880646\n",
      "Step 720, Loss: 1.0117523670196533\n",
      "Step 730, Loss: 0.9182001352310181\n",
      "Step 740, Loss: 0.9510810375213623\n",
      "Step 750, Loss: 0.7725158929824829\n",
      "Step 760, Loss: 1.9429140090942383\n",
      "Step 770, Loss: 1.0451724529266357\n",
      "Step 780, Loss: -2.0005581378936768\n",
      "Epoch 1 - Average Loss: -0.0127, Accuracy: 0.1010, Precision: 0.1000, Recall: 0.2550, F1-Score: 0.1436\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15080\\182075848.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Load CIFAR-10 dataset for training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_ds_cifar10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_combined_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cifar10\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifar10_label_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Train on CIFAR-10 (CIFAR-100 can be loaded similarly)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds_cifar10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15080\\1562859038.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(train_ds, epochs, num_classes)\u001b[0m\n\u001b[0;32m     51\u001b[0m               \u001b[1;33mf\"\u001b[0m\u001b[1;33mRecall: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mepoch_recall\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m.4f\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m, \u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m               \u001b[1;33mf\"\u001b[0m\u001b[1;33mF1-Score: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mepoch_f1_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m.4f\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# Save metrics to DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         metrics_df = metrics_df.append({\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[1;34m\"Epoch\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;34m\"Loss\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch_accuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths to label files\n",
    "    cifar10_label_file = \"./data/CIFAR-10_human_ordered.npy\"\n",
    "    cifar100_label_file = \"./data/CIFAR-100_human_ordered.npy\"\n",
    "\n",
    "    # Load CIFAR-10 dataset for training\n",
    "    train_ds_cifar10 = load_combined_dataset(\"cifar10\", cifar10_label_file, num_classes=10)\n",
    "\n",
    "    # Train on CIFAR-10 (CIFAR-100 can be loaded similarly)\n",
    "    train_model(train_ds_cifar10, epochs=10, num_classes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
